## Вариационный байесовский вывод

#### Описание и обоснование

Пусть рассматривается задача байесовского статистического вывода, где по матрице наблюдаемых переменных $X$ требуется найти апостериорное распределение матрицы скрытых переменных $p(Z \, \vert \, X)$, имея явный вид вероятностной модели $p(X, Z) = p(X \, \vert \, Z) p(Z)$, где оба множителя правой части даны аналитически.

Вариационный байесовский вывод применяется в ситуации, когда апостериорное распределение $p(Z \, \vert \, X) = \frac{p(X, Z)}{p(X)} = \frac{p(X, Z)}{\int p(X, Z) dZ}$ не получается найти явно из-за того, что не вычисляется аналитически $p(X) = \int p(X, Z) dZ$. Вместо искомого распределения $p(Z \, \vert \, X)$ ищется его некоторое приближение $q(Z)$, тоже являющееся вероятностным распределением матрицы $Z$. Итак, цель заключается в том, чтобы найти $q(Z) \approx p(Z \, \vert \, X)$, где близость понимается в смысле минимизации дивергенции Кульбака-Лейблера между распределениями $q(Z)$ и $p(Z \, \vert \, X)$:
$$D_{KL}(q(Z) \, \Vert \, p(Z \, \vert \, X)) \to \min_{q},$$
причём порядок аргументов дивергенции Кульбака-Лейблера такой, какой есть, потому что именно для такой дивергенции Кульбака-Лейблера удаётся найти решение сформулированной оптимизационной задачи.

Для решения поставленной задачи сведём её к эквивалентной ей оптимизационной задаче.

Каким бы ни было распределение $q(Z)$, всегда можно переписать логарифм правдоподобия наблюдаемой части выборки следующим образом:
$$\log p(X) = \log p(X) \int q(Z) dZ = \int q(Z) \log p(X) dZ = \int q(Z) \log \frac{p(X, Z)}{p(Z \, \vert \, X)} dZ = \int q(Z) \log \frac{p(X, Z)q(Z)}{p(Z \, \vert \, X)q(Z)} dZ \\ = \int q(Z) \log \frac{p(X, Z)}{q(Z)} dZ + \int q(Z) \log \frac{q(Z)}{p(Z \, \vert \, X)} dZ.$$

Обозначим первое слагаемое последнего выражения за $\mathcal{L}(q)$ и заметим, что второе слагаемое является той самой дивергенцией Кульбака-Лейблера $D_{KL}(q(Z) \, \Vert \, p(Z \, \vert \, X))$, которую необходимо минимизировать. Значит,
$$\log p(X) = \mathcal{L}(q) + D_{KL}(q(Z) \, \Vert \, p(Z \, \vert \, X)).$$

Благодаря такому равенству можно взглянуть на поиск приближения $q(Z)$ для неизвестного распределения $p(Z \, \vert \, X)$, которое, собственно говоря, неизвестно из-за невозможности вычислить $p(X)$, с двух точек зрения:

* Исходная оптимизационная задача. Минимизацией $D_{KL}(q(Z) \, \Vert \, p(Z \, \vert \, X))$ по $q$ можно получить приближение для $p(Z \, \vert \, X)$, потому что дивергенция Кульбака-Лейблера неотрицательна и, более того, обнуляется тогда и только тогда, когда два распределения идентичны.

* Эквивалентная ей оптимизационная задача. Максимизацией $\mathcal{L}(q)$ по $q$ можно получить приближение (а именно оценку снизу) для $p(X)$, потому что $\mathcal{L}(q)$ не может быть больше $\log p(X)$ в силу неотрицательности дивергенции Кульбака-Лейблера;

Раз две выписанных оптимизационных задачи эквивалентны (имеют одинаковое решение), остановимся на второй:
$$\mathcal{L}(q) \to \max_{q}.$$
Она выбрана, потому что удобнее для применения вариационного подхода.

Заметим, что до сих пор не вводилось никаких дополнительных предположений по сравнению с теми, что есть в общей постановке задачи байесовского статистического вывода. Вовлечение вариационного подхода опирается на то, что на распределение $q$ накладывается ограничение факторизуемости. Оно означает, что существует такая разбивка столбцов матрицы $Z$ на $m$ непересекающихся множеств, что
$$q(Z) = \Pi_{i=1}^m q_i(Z_i),$$
где $Z_i$ — матрица, получающаяся из матрицы $Z$ оставлением тех и только тех столбцов, которые попадают в $i$-е множество, а $q_i(Z_i)$ — произвольное распределение матрицы $Z_i$. Стало быть, теперь ищется приближение распределения $p(Z \, \vert \, X)$ в классе факторизуемых вероятностных распределений.

Будем решать оптимизационную задачу итеративно методом покоординатного подъёма. Инициализируем как-то все $q_i$, а затем на каждом шаге будем по очереди обновлять каждое из них в соответствии со следующим правилом:
$$q_i(Z_i) := \frac{\exp(\int \log p(X, Z) \Pi_{j \ne i} q_j(Z_j) dZ_{-i})}{\int \exp(\int \log p(X, Z) \Pi_{j \ne i} q_j(Z_j) dZ_{-i}) dZ_i},$$
где $Z_{-i}$ обозначает матрицу, получающуюся из $Z$ выкидыванием столбцов, принадлежащих $i$-му множеству. Выписанная формула обновления $q_i(Z_i)$ называется основной формулой вариационного байесовского вывода, а её получение здесь ради краткости опущено. Вывод же назван вариационным, потому что при оптимизации используется взятие вариации оптимизируемого функционала $\mathcal{L}(q)$ вдоль направления, соответствующего $i$-му множеству скрытых переменных. 

Может показаться, что интеграл из знаменателя правой части основной формулы вариационного байесовского вывода ничуть не проще исходного интеграла из формулы для апостериорного распределения. Однако взять этот интеграл иногда удаётся. В следующем разделе будет описано, что и при каких условиях возможно.

#### Применимость в зависимости от имеющихся свойств

Чтобы описывать различные ситуации, введём ряд определений.

Начнём с понятия условной сопряжённости, являющегося ослаблением понятия сопряжённости. Пусть матрица $Z$ факторизуется в разбивке своих столбцов на $m$ непересекающихся множеств. Примем за $\{Z_i\}_{i=1}^m$ множество из $m$ матриц, $i$-я из которых получается из $Z$ оставлением только соответствующих $i$-му множеству столбцов, а через $Z_{-i}$ обозначим матрицу, получающуюся из $Z$ выкидыванием всех столбцов, соответствующих $i$-му множеству. Будем говорить, что присутствует условная сопряжённость, если одновременно выполнены $m$ условий, $i$-е из которых имеет вид:

* Для любого зафиксированного значения матрицы $Z_{-i}$ (то есть для любых зафиксированных значений всех матриц $Z_j$, где берутся все $j$ от 1 до $m$ кроме $i$), распределения $p(X \, \vert Z_i, Z_{-i})$ и $p(Z_i \, \vert Z_{-i})$ сопряжены друг к другу.

Теперь предположим, что матрица $Z$ является боковой конкатенацией матриц $T$ и $\Theta$, где $T$  матрица скрытых признаков объектов, а $\Theta$ — матрица неизвестных параметров вероятностной модели. Будем считать, что $p(Z) = p(T, \Theta) = p(T)p(\Theta)$, то есть имеется факторизация при разбивке столбцов $Z$ на те, которые относятся к $T$, и на те, которые относятся к $\Theta$.

Определим $T$-сопряжённость как ослабление условной сопряжённости до того, что при факторизации в разбивке $Z$ на $T$ и $\Theta$ условие выполнено только для $Z_i = T$, то есть как то, что пару сопряжённых распределений образуют $p(X \, \vert T, \Theta)$ и $p(T \, \vert \, \Theta)$. Аналогично, определим $\Theta$-сопряжённость как то, что сопряжены друг к другу распределения $p(X \, \vert \Theta, T)$ и $p(\Theta \, \vert \, T)$.

Далее, назовём условной $T$-сопряжённостью свойство, при котором $T$ сама как-то факторизуется и для факторизации всей $Z$ на $\Theta$ и на то, на что разбивается $T$, есть условная сопряжённость, а условной $\Theta$-сопряжённостью назовём свойство, при котором $\Theta$ как-то факторизуется и для факторизации всей $Z$ на $T$ и на то, на что разбивается $\Theta$, есть условная сопряжённость. 

В зависимости от присутствующего свойства применимы следующие методы статистического вывода:

* сопряжённость (полная, т.е. $Z$-сопряжённость): полный байесовский вывод, без каких бы то ни было приближений оценивающий $p(Z \, \vert X) = p(T, \Theta \, \vert \, X)$;

* условная сопряжённость в разбивке $Z$ на $T$ и $\Theta$: вариационный байесовский вывод, оценивающий $q(Z) = q(T)q(\Theta) \approx p(T, \Theta \, \vert \, X)$;

* $T$-сопряжённость: точный EM-алгоритм, являющийся частным случаем вариационного байесовского вывода, где дополнительно предполагается, что $q(\Theta) = \delta(\Theta - \Theta_0)$ и в этой записи $\delta$ — дельта-функция Дирака, а $\Theta_0$ — константная матрица, которую требуется найти; точный EM-алгоритм оценивает $q(Z) = q(T)\delta(\Theta - \Theta_0) \approx p(T, \Theta \, \vert \, X)$;

* $\Theta$-сопряжённость: точный ME-алгоритм, являющийся частным случаем вариационного байесовского вывода, где дополнительно предполагается, что $q(T) = \delta(T - T_0)$, а $T_0$ — константная матрица, которую требуется найти; точный ME-алгоритм оценивает $q(Z) = \delta(T - T_0)q(\Theta) \approx p(T, \Theta \, \vert \, X)$;

* условная $T$-сопряжённость: вариационный EM-алгоритм, оценивающий $q(Z) = \Pi_{i=1}^m q_i(T_i) \, \delta(\Theta - \Theta_0) \approx p(T, \Theta \, \vert \, X)$;

* условная $\Theta$-сопряжённость: вариационный ME-алгоритм, оценивающий $q(Z) = \delta(T - T_0) \, \Pi_{i=1}^m q_i(\Theta_i)\approx p(T, \Theta \, \vert \, X)$;

* никаких свойств нет: жёсткий EM-алгоритм, оценивающий $q(Z) = \delta(T - T_0) \delta(\Theta - \Theta_0) \approx p(T, \Theta \, \vert \, X)$, то есть дающий только точечные оценки.

Получившийся список можно прокомментировать так. Чем более слабое свойство имеется, тем более "простым" распределением приходится полагать то распределение, которое ищется в задаче статистического вывода. "Простота" здесь понимается как факторизуемость.

Напоследок разберём какой-нибудь пример, показывающий, почему указанных свойств достаточно для соответствующих методов. Скажем, остановимся на достаточности $T$-сопряжённости для точного EM-алгоритма. В точном EM-алгоритме сложное место находится на E-шаге, где ищется распределение $$q^{(n+1)}(Z) = p(Z \, \vert \, X, \Theta^{(n)}) = \frac{p(X, Z \, \vert \,  \Theta^{(n)})}{\int p(X, Z \, \vert \, \Theta^{(n)}) dZ} = \frac{p(X \, \vert \, Z, \Theta^{(n)})p(Z \, \vert \, \Theta^{(n)})}{\int p(X \, \vert \, Z, \Theta^{(n)})p(Z \, \vert \, \Theta^{(n)}) dZ} = \frac{p(X \, \vert \, T, \Theta^{(n)})p(T \, \vert \, \Theta^{(n)})\delta(\Theta - \Theta^{(n)})}{\int p(X \, \vert \, T, \Theta^{(n)})p(T \, \vert \, \Theta^{(n)})\delta(\Theta - \Theta^{(n)}) dT d\Theta} = \frac{p(X \, \vert \, T, \Theta^{(n)})p(T \, \vert \, \Theta^{(n)})}{\int p(X \, \vert \, T, \Theta^{(n)})p(T \, \vert \, \Theta^{(n)}) dT} = \frac{p(X, T \, \vert \, \Theta^{(n)})}{\int p(X, T \, \vert \, \Theta^{(n)}) dT},$$
и тут интеграл в полученном в конце выражении берётся явно в силу $T$-сопряжённости.
