## Декомпозиция построения и обучения нейронной сети

В построении и обучении нейронных сетей есть три ключевых составляющих:

* __Модель__, то есть архитектура и структура нейронной сети:
    - Сколько слоёв?
    - Какой ширины каждый из слоёв?
    - Какие нейроны (юниты) используются?
        - обычные,
        - с пакетной нормализацией,
        - LSTM-модули,
        - etc;
    - Какие связи между слоями?
        - полные,
        - локальные (то есть каждый нейрон связан лишь с некоторой областью предыдущего слоя),
        - свёрточные (похожи на локальные, но веса в фильтре одинаковые для всего слоя),
        - рекуррентные,
        - etc;
    - Какие функции активации используются в каждом из слоёв?
        - нет активации,
        - сигмоида,
        - гиперболический тангенс,
        - ReLU и его модификации, такие как Leaky ReLU или PReLU,
        - etc.
    - Есть ли какие-либо связанные веса (shared weights)?


* __Функция потерь__ (должна быть дифференцируемой почти везде):
    - Для регрессии:
        - MSE, среднеквадратичная ошибка,
        - MAE, средний модуль ошибки,
        - etc.
    - Для классификации:
        - логарифмическая функция потерь (связана с понятием кросс-энтропии, её минимизация эквивалентна минимизации дивергенции Кульбака-Лейблера),
        - etc.
    - Для других задач:
        - GAN loss,
        - etc.


* __Оптимизатор__, то есть способ по данным настроить все параметры сети:
    - На базе градиентного спуска с таким приёмом, как обратное распространение ошибок (этот приём сильно ускоряет вычисление градиента):
        - Когда обновлять веса?
            - После подсчёта коррекций на всех объектах обучающей выборки (классический градиентный спуск),
            - После подсчёта коррекций на всём пакете из случайно выбранных объектов обучающей выборки (пакетный градиентный спуск),
            - После подсчёта коррекций на каждом отдельно взятом объекте (стохастический градиентный спуск).
        - Какой темп обучения (learning rate) выбрать?
            - Постоянный,
            - С расписанием, то есть в виде заранее заданной невозрастающей функции от числа шагов,
            - Адаптивный:
                - AdaGrad,
                - RMSProp,
                - Adam.
        - Какую инерцию (momentum) обновлений весов выбрать?
            - никакую,
            - обычную,
            - инерцию Нестерова.
        - Как инициализировать веса каждого из слоёв перед началом обучения?
            - Случайно породить $I \times O$ весов, где $I$ — размер входа слоя, а $O$ — размер выхода слоя:
                - Из гауссовского распределения с нулевым средним и дисперсией:
                    - 0.01 (или ещё меньше, если всё равно обучение не идёт),
                    - $2 / (I + O)$ (Glorot Normal Initialization),
                    - $1/I$ (часто используется для функции активации $\tanh$),
                    - $2/I$ (He Normal Initialization, часто используется для функции активации ReLU);
                - Из равномерного распределения на отрезке $[-a, a]$, где $a$ равно:
                    - $\sqrt{6 / (I + O)}$ (Glorot Uniform Initialization),
                    - $\sqrt{3 / I}$ (LeCun Uniform Initialization).
            - Жадно послойно предобучить с дополнением до автокодировщика.
        - Какие техники регуляризации использовать?
            - $L_1$-регуляризация (приводит к отбору весов),
            - $L_2$-регуляризация (приводит к затуханию весов),
            - дропаут,
            - подмешивание шума.
        - Использовать ли раннюю остановку, отталкивающуюся от оценки качества на отложенной выборке?
    - На базе генетических алгоритмов (не мэйнстрим в обучении нейронных сетей).
