## Автокодировщики с большей размерностью скрытого слоя

Рассмотрим автокодировщик (autoencoder) со входным слоем размерности $l$, одним скрытым слоем размерности $h$ и выходным слоем, размерность которого по определению должна быть равна размерности входного слоя, то есть $l$. Вопрос: могут ли для чего-то пригодиться автокодировщики, где $h > l$?

Без дополнительных модификаций такие автокодировщики не смогут обучаться чему-то нетривиальному, потому что всегда есть вырожденное решение: использовать лишь $l$ нейронов из $h$, чтобы просто передать вход в неизменном виде.

Есть следующие способы заставить скрытый слой выучивать нетривиальное представление:

* К ошибке восстановления, на входном векторе $x$ равной $\Vert x - \mathrm{autoencoder}(x)\Vert_2$, добавить регуляризатор, подталкивающий к разреженности скрытого слоя: например, $L_1$-норму $h$-мерного вектора, получающегося из $x$ в скрытом слое;

* Ко входу подмешивать шум, но при вычислении ошибки восстановления требовать близости к незашумлённому входу, то есть минимизировать математическое ожидание функции, на входном векторе $x$ равной $\Vert x - \mathrm{autoencoder}(x + \varepsilon)\Vert_2$, где $\varepsilon$ — шум.

* Каждую из компонент вектора входов с некоторой вероятностью $p$ обнулять, но при вычислении ошибки восстановления сверяться с вектором входов, где все компоненты имеют исходный вид — так автокодировщик будет побуждён выучивать взаимосвязи между признаками, позволяющие по части признаков восстановить остальные признаки. 

Полученный автокодировщик можно использовать в следующих целях:

* То, что получается в скрытом слое, можно брать в качестве признакового описания объекта; такие признаки могут содержать в себе более сложные взаимодействия, чем признаки, даваемые автокодировщиком с низким $h$; пригождается это в частичном обучении (semi-supervised learning), ведь объекты без меток тоже влияют на выучиваемый способ порождать признаки;

* До появлении пакетной нормализации и остаточного обучения была популярна "жадная" (послойная от входов к выходам) инициализация весов глубокой однонаправленной нейронной сети, при которой инициализируемый на данный момент слой дополнялся до автокодировщика и обучался задаче автокодировщика, где кодировалось то, что подавал на вход предыдущий слой, — стало быть, если в каком-то месте ширина нейронной сети возрастала, то требовалось обучать автокодировщик, у которого $h > l$.
