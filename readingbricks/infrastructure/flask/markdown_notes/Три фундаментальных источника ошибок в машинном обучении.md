## Три фундаментальных источника ошибок в машинном обучении

Оптимальный в байесовском смысле регрессор (или классификатор) — это то, что задаётся следующим формальным выражением:
$$f_{opt} = \arg \min_{f \in F} \int_{(x, y) \sim P_{true}(x, y)}l(f(x), y),$$
где $F$ — множество всех возможных регрессоров (или классификаторов соответственно), $P_{true}$ — истинное совместное распределение признаков и целевой переменной, а $l$ — функция потерь, по значению регрессора (классификатора) на векторе признаков и целевой переменной на этом векторе признаков возвращающая соответствующий штраф.

В реальности есть ограничения, не позволяющие найти $f_{opt}$.

Во-первых, неизбежна ошибка приближения (approximation error): поиск проводится не по всему множеству возможных регрессоров (классификаторов), а лишь по какому-то подмножеству $\overline{F} \subset F$, например, по некому параметрическому семейству. Скажем, в случае с линейной регрессией $\overline{F}$ — это множество всех линейных регрессоров. Как следствие, ищется уже нечто другое:
$$\hat{f_1} = \arg \min_{f \in \overline{F}} \int_{(x, y) \sim P_{true}(x, y)}l(f(x), y).$$
Ошибка, вызываемая отличием $\hat{f_1}$ от $f_{opt}$, и называется ошибкой приближения.

Во-вторых, существует ошибка оценивания по данным (estimation error): невозможно получить истинную генеральную совокупность $P_{true}$, а вместо неё есть лишь выборка конечного размера $N$. С учётом такого ограничения задача принимает вид:
$$\hat{f_2} = \arg \min_{f \in \overline{F}} \frac{1}{N} \sum_{i=1}^{N} l(f(x_i), y_i) + \alpha R(f),$$
где $(x_i, y_i)$ — признаки и ответ на $i$-м объекте обучающей выборки, $R(f)$ — регуляризатор, а $\alpha$ — коэффициент, задающий силу регуляризации. Ошибка оценивания — ошибка, вызываемая отличием $\hat{f_2}$ от $\hat{f_1}$.

В-третьих, не всегда удаётся довести поиск минимума из предыдущего пункта до конца, и отсюда возникает ошибка оптимизации (optimization error). Если предположить, что вычислительные ресурсы ограничены и есть всего лишь $T$ итераций, то получится, что решается задача
$$\hat{f_3}^{(T)} = \arg \min_{(T); f \in \overline{F}} \frac{1}{N} \sum_{i=1}^{N} l(f(x_i), y_i) + \alpha R(f),$$
где запись $(T)$ под минимумом обозначает, что минимум ищется за ограниченное число шагов. По аналогии с определением предыдущих ошибок ошибка оптимизации — это ошибка, вызываемая отличием $\hat{f_3}^{(T)}$ от $\hat{f_2}$.

Можно считать, что машинное обучение на больших данных отличается от машинного обучения на маленьких данных тем, что ошибка оптимизации начинает доминировать над ошибкой оценивания по данным.

Подробности есть в статье [Bottou, Bousquet, 2007](https://papers.nips.cc/paper/3323-the-tradeoffs-of-large-scale-learning.pdf).
