## Внутреннее устройство LSTM-нейрона

#### Что означает LSTM?

Аббревиатура LSTM расшифровывается как Long Short-Term Memory (краткосрочная память произвольной длины). Так называется архитектура нейронной сети, часто используемая при решении задач, где объекты образуют последовательность, причём влияние некоторого объекта может распространяться на целевые переменные объектов, стоящих впереди него на произвольном расстоянии.

#### Содержимое LSTM-нейрона

Отличием LSTM является то, что в ней вводится специальный нейрон со сложной внутренней структурой, называемый LSTM-нейроном или LSTM-модулем (LSTM cell, LSTM unit, LSTM neuron). Рассмотрим устройство такого нейрона.

Для каждого индекса $t$, которым нумеруются позиции последовательности, с LSTM-нейроном ассоциированы следующие сущности:

* входы и выходы:
    - вектор входов $x_t$, берущийся из последовательности и имеющий длину $d$,
    - вектор возвращаемых нейроном выходов $y_t$, имеющий длину $h$;

* внутреннее состояние:
    - вектор "памяти" $c_t$ той же длины $h$, что и вектор выходов;

* три вектора, называемых вентилями и имеющих ту же длину $h$, что и выход:
    - входной вентиль (input gate), $i_t$,
    - вентиль забвения (forget gate), $f_t$,
    - выходной вентиль (output gate), $o_t$;

* восемь матриц весов и четыре векторных свободных члена:
    - $W_i$, $W_f$, $W_o$, $W_c$, их размер $h \times d$,
    - $U_i$, $U_f$, $U_o$, $U_c$, их размер $h \times h$,
    - $b_i$, $b_f$, $b_o$, $b_c$, их длина $h$.
    
Когда нейронная сеть уже обучена, последние двенадцать сущностей не меняются от шага к шагу, и именно поэтому индекс $t$ у них отсутствует.

#### Динамика при движении по последовательности

Начальные условия в позиции $t = 0$, предшествующей началу последовательности, задаются так:

* $c_0 = 0$, $y_0 = 0$,

* веса матриц и свободных членов как-то инициализированы, если происходит обучение, или уже обучены и зафиксированы, если речь идёт о применении,

* чему равны вентили, неважно.

При заданной последовательности $x_t$ для любой позиции $t \ge 1$ можно рекуррентно определить значения всех сущностей, ассоциированных с LSTM-нейроном, по формулам:
$$i_t = \sigma(W_i x_t + U_i y_{t-1} + b_i),$$
$$f_t = \sigma(W_f x_t + U_f y_{t-1} + b_f),$$
$$o_t = \sigma(W_o x_t + U_o y_{t-1} + b_o),$$
$$c_t = f_t \circ c_{t-1} + i_t \circ \tanh (W_c x_t + U_c y_{t-1} + b_c),$$
$$y_t = o_t \circ \tanh (c_t),$$
где $\sigma$ — сигмоидная функция активации, а $\circ$ — операция взятия адамарова произведения (поэлементное умножение).

#### Интерпретация

Почему данная конструкция, после обучения являющаяся некоторой динамической системой в дискретном времени, осмысленна?

Можно считать, что LSTM-нейрон в зависимости от значений вентилей способен демонстрировать различные режимы поведения (ниже 0 и 1 обозначают векторы из одних 0 или 1 соответственно):

* при $f_t = 0$, $i_t = o_t = 1$ получается обычный рекуррентный нейрон (нейрон Элмана);

* при $o_t = 0$, $i_t = f_t = 1$ происходит накопление входной информации (например, нейрон может работать счётчиком), а на выход информация не отдаётся;

* при $i_t = o_t = 0$, $f_t = 1$ происходит хранение того, что в памяти, без изменений;

* при $i_t = 0$, $f_t = o_t = 1$ происходит извлечение наружу информации, накопившейся в памяти;

* при $i_t = f_t = 0$ происходит сбрасывание памяти.

Таким образом, LSTM-нейрон обладает способностями к гибкому управлению памятью и способен проводить над ней все операции, кажущиеся необходимыми. Если данных много и закономерности в них устойчивые, то нейронная сеть с такими нейронами сможет выучить их, даже в случае отсутствия ограничения максимальной дальности влияния входов на выходы.

#### Обучение

Обучение LSTM-нейрона производится методом, по-английски называемым Truncated Back-Propagation Through Time, где первое слово обозначает, что в вычислениях следующие частные производные полагаются равными нулю:
$$\frac{\partial i_t}{\partial y_{t-1}} = \frac{\partial f_t}{\partial y_{t-1}} = \frac{\partial o_t}{\partial y_{t-1}} = \frac{\partial c_t}{\partial y_{t-1}} = 0.$$

#### Модификации

Помимо классического LSTM-нейрона, описанного выше, существует версия, называемая LSTM с глазками (peepholes). Её отличие в том, что вентили могут обращаться к памяти нейрона (подглядывать в неё через глазки). Вводятся ещё три матрицы размера $h \times h$, обозначаемые как $V_i$, $V_f$ и $V_o$. В рекуррентных соотношениях, задающих динамику при движении по последовательности, последние два равенства остаются неизменными, а первые три принимают вид:
$$i_t = \sigma(W_i x_t + U_i y_{t-1} + V_i c_{t-1} + b_i),$$
$$f_t = \sigma(W_f x_t + U_f y_{t-1} + V_f c_{t-1} + b_f),$$
$$o_t = \sigma(W_o x_t + U_o y_{t-1} + V_o c_{t} + b_o).$$
В последнем из соотношений используется $c_t$, а не $c_{t-1}$, потому что на момент обновления значения выходного вентиля значение памяти на текущем шаге уже известно.

Есть и версия LSTM с глазками, где матриц $U_i$, $U_f$, $U_o$ и $U_c$ нет (т.е. можно считать, что они тождественно нулевые).
