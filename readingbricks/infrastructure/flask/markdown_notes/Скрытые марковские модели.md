## Скрытые марковские модели

#### Введение

Скрытая марковская модель с $m$ скрытыми состояниями и $n$ наблюдаемыми состояниями — это совокупность сущностей из следующего списка:

* Вероятностное распределение $\pi$ на множестве $\{1, ..., m\}$, представляющее собой распределение скрытого состояния в начальный момент времени;

* Матрица переходов между скрытыми состояниями $A$, её размер $m \times m$, а на пересечении $i$-й строки и $j$-го столбца стоит вероятность из скрытого состояния $i$ перейти в скрытое состояние $j$; как следствие сумма элементов любой строки матрицы $A$ равна 1;

* Матрица эмиссии наблюдаемых значений $B$, её размер $m \times n$, а на пересечении $i$-й строки и $j$-го столбца стоит условная вероятность того, что текущее наблюдаемое состояние равно $j$ при условии, что текущее скрытое состояние есть $i$; сумма элементов любой строки матрицы $B$ тоже равна 1.

Для переноса на случай, когда наблюдаемые значения непрерывны, можно считать, что вместо матрицы $B$ имеются $m$ вероятностных распределений.

В контексте скрытых марковских моделей существуют следующие задачи:

* Зная $\pi$, $A$ и $B$, вычислить вероятность данной последовательности наблюдаемых значений $(x_t)_{t=1}^T$, где $T$ — длина последовательности;

* Зная $\pi$, $A$ и $B$, по имеющейся последовательности наблюдаемых значений $(x_t)_{t=1}^T$ восстановить последовательность скрытых значений $(h_t)_{t=1}^T$;

* Имея множество последовательностей наблюдаемых значений $\left\{\left. (x_t)_{t=1}^{T_k} \; \right\vert \; k \in \{1, ..., c\}\right\}$, где $c$ — количество обучающих последовательностей, оценить $\pi$, $A$ и $B$, то есть обучить скрытую марковскую модель.

#### Задача вычисления вероятности наблюдаемой последовательности

Эта задача является задачей вероятностного (прямого) вывода, а не задачей статистического (обратного) вывода. На самом деле, подсчитать искомую вероятность можно просто в лоб по определению скрытой марковской модели:
$$p\!\left((x_t)_{t=1}^T\right) = \sum_{(h_t)_{t=1}^T \in \{1, ..., M\}^T} \pi(h_1) B_{h_1x_1} \Pi_{t=2}^T A_{h_{t-1}h_t} B_{h_tx_t},$$
однако такой подход имеет экспоненциальную от длины наблюдаемой последовательности $T$ сложность, потому что суммирование проводится по $M^T$ возможным вариантам последовательностей скрытых состояний.

Для того чтобы задача была вычислительно подъёмной, используется алгоритм прохода вперёд (forward algorithm), восходящий к динамическому программированиню.

Введём индексируемые позицией в последовательности $t$ и скрытым состоянием $i$ переменные
$$\alpha_{ti} = p\!\left((x_s)_{s=1}^t, h_t = i\right).$$
Иными словами, $\alpha_{ti}$ есть совместная вероятность появления подпоследовательности $(x_s)_{s=1}^t$ на первых $t$ наблюдаемых позициях и события, заключающегося в том, что в $t$-й позиции скрытое состояние равнялось $i$.

Теперь можно описать алгоритм прохода вперёд:
1. Для всех $i \in \{1, ..., m\}$ вычислить начальные значения:
$$\alpha_{1i} = \pi(i)B_{ix_1}.$$
2. Для всех $(t, i) \in \{2, ..., T\} \times \{1, ..., m\} $ в порядке неубывания $t$ провести индуктивный шаг вперёд:
$$\alpha_{ti} = \sum_{j=1}^m \alpha_{(t-1)j}A_{ji}B_{ix_t}.$$
3. Вычислить искомую вероятность:
$$p\!\left((x_t)_{t=1}^T\right) = \sum_{i=1}^m \alpha_{Ti}.$$

#### Задача восстановления последовательности скрытых состояний

Эта задача решается алгоритмом Витерби, который, как и алгоритм прохода вперёд, тоже восходит к динамическому программированию.

Введём индексируемые позицией в последовательности $t$ и скрытым состоянием $i$ переменные
$$\psi_{ti} = \max_{(h_s)_{s=1}^t \in \{1, ..., m\}^t}p\!\left((x_s)_{s=1}^t, h_t = i, (h_s)_{s=1}^t\right),$$
каждая из которых интерпретируется как максимальная совместная вероятность появления известной наблюдаемой подпоследовательности $(x_s)_{s=1}^t$, появления произвольной скрытой подпоследовательности $(h_s)_{s=1}^t$ и события, заключающегося в том, что в $t$-й позиции скрытое состояние равнялось $i$.

Также введём индексируемые позицией в последовательности $t$ и скрытым состоянием $i$ переменные
$$\phi_{ti} = \left(\arg\max_{(h_s)_{s=1}^t \in \{1, ..., m\}^t}p\!\left((x_s)_{s=1}^t, h_t = i, (h_s)_{s=1}^t\right)\right)_{t-1},$$
каждая из которых интерпретируется как скрытое состояние в позиции $t-1$ в подпоследовательности скрытых состояний, максимизирующей $\psi_{ti}$.

Алгоритм Витерби устроен так:
1. Для всех $i \in \{1, ..., m\}$ вычислить начальные значения:
$$\psi_{1i} = \pi(i)B_{ix_1},$$
$$\phi_{1i} = 0,$$
где 0 взят как произвольный некорректный номер состояния (номера состояний начинаются с 1). Иными словами, ни одну $\phi_{1i}$ инициализировать не обязательно.
2. Для всех $(t, i) \in \{2, ..., T\} \times \{1, ..., m\} $ в порядке неубывания $t$ провести индуктивный шаг вперёд:
$$\psi_{ti} = B_{ix_t} \max_{j \in \{1, ..., m\}}A_{ji}\psi_{(t-1)j},$$
$$\phi_{ti} = \arg\max_{j \in \{1, ..., m\}}A_{ji}\psi_{(t-1)j}.$$
3. Восстановление последнего скрытого состояния:
$$h_T = \arg\max_k \psi_{Tk}.$$
4. Восставновление всех остальных скрытых состояний индуктивными шагами назад:
$$h_t = \phi_{(t+1)h_{t+1}}.$$

Полученная алгоритмом Витерби последовательность $(h_t)_{t=1}^T$ является последовательностью с наибольшей совместной вероятностью $p\!\left((h_t)_{t=1}^T, (x_t)_{t=1}^T\right)$ и, как следствие, с наибольшей условной вероятностью $p\!\left(\left.(h_t)_{t=1}^T \right| (x_t)_{t=1}^T\right)$.

#### Задача обучения скрытой марковской модели

Есть несколько подходов к обучению скрытых марковских моделей:

* Алгоритм Баума-Уэлша, являющийся частным случаем EM-алгоритма, применяемого во многих задачах обучения вероятностных графических моделей;

* Градиентный спуск в пространстве троек $(\pi, A, B)$, после каждого шага которого строки матриц нормируются, чтобы их сумма, как это и требуется, равнялась 1.

Помимо батчевых версий указанных алгоритмов обучения, сразу обучающихся на всём множестве имеющихся последовательностей без возможности дальнейшего дообучения, существуют и онлайновые версии, способные дообучаться на одной или более последовательностях.
