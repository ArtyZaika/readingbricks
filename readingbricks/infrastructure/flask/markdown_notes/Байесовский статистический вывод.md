## Байесовский статистический вывод

#### Задача статистического вывода

Пусть даны следующие сущности:

* выборка, то есть матрица объекты-признаки $X$, содержащая только наблюдаемые признаки объектов;

* вероятностная модель, то есть аналитический вид совместного распределения $p(X, Z)$, где $Z$ — матрица со скрытыми признаками объектов.

Комментарии по поводу того, что дано, таковы. Матрицы $X$ и $Z$ имеют одинаковое количество строк, но, разумеется, количество столбцов не обязано быть одинаковым. Плотность же вероятностного распределения дана именно для совокупности объектов (для матриц, где может быть более одной строки), а не для одного объекта, потому что объекты могут не быть независимы друг от друга: например, ими могут быть наблюдения элементов одной и той же последовательности с влиянием прошлого на будущее.

Задача заключается в нахождении оценки $\hat{Z}$ для неизвестной матрицы $Z$, соответствующей известной матрице $X$. Также задачу можно расширить до такой: найти апостериорное распределение $p(Z \, \vert \, X)$, а не только точечную оценку $\hat{Z}$.

#### Байесовский подход к задаче статистического вывода

При байесовском подходе предполагается, что $p(X, Z) = p(X \, \vert \, Z) p(Z)$, где в аналитическом виде даны и $p(X \, \vert \, Z)$, и $p(Z)$. При этом $p(X \, \vert \, Z)$ называется правдоподобием выборки $X$ при условии $Z$, а $p(Z)$ является априорным распределением $Z$ (взятым на основании знаний о предметной области или полученным как апостериорное распределение $Z$ при условии какой-то ранее наблюдавшейся выборки $X_\mathrm{old}$).

Решение задачи, данной в байесовской постановке, описывается так:

* Апостериорное распределение находится по теореме Байеса:
$$p(Z \, \vert \, X) = \frac{p(X, Z)}{p(X)} = \frac{p(X, Z)}{\int p(X, Z) dZ},$$

* Точечная оценка $\hat{Z}$ получается как мода или как математическое ожидание полученного апостериорного распределения:
    - $\hat{Z} = \arg \max_Z p(Z \, \vert \, X)$,
    - $\hat{Z} = \mathbb{E}_{p(Z \, \vert \, X)}[Z]$.
    
Основная сложность, связанная с байесовским статистическим выводом, как правило, заключается во взятии интеграла из знаменателя правой части формулы для апостериорного распределения.

Дополнительно отметим ещё одну особенность байесовского взгляда на описанную задачу. В байесовской парадигме неизвестные параметры вероятностного распределения являются случайными величинами. Поэтому в качестве матрицы $Z$ можно брать матрицу параметров распределения, которые требуется оценить. Если параметры не могут меняться от наблюдения к наблюдению, необходимо наложить на $Z$ ограничение, что все строки являются одинаковыми, — такое ограничение закладывается в априорное распределение $Z$ и поэтому отдельно оно нигде не фигурирует. Формально это ограничение вводится так: от $p(Z)$ требуется быть нулевой для любой матрицы $Z$, у которой есть различающиеся строки. 

#### Способы решения задачи

Существуют следующие способы найти апостериорное распределение $p(Z \, \vert \, X)$:

* Если это возможно, явно взять интеграл из знаменателя в выражении для апостериорного распределения. Данный подход примением, например, тогда, когда $Z$ принимает дискретные значения и интеграл, на самом деле, является суммой, или тогда, когда $p(X \, \vert \, Z)$ и $p(Z)$ — пара сопряжённых (conjugate) распределений;

* Вариационный байесовский вывод. Вводится предположение факторизуемости $p(X, Z)$ на непересекающиеся группы переменных, и на базе этого предположения проводится приближённое вычисление апостериорного распределения методом покоординатного подъёма;

* Методы Монте-Карло по схеме марковской цепи, MCMC. Это численные методы взятия интеграла из знаменателя в выражении для апостериорного распределения, позволяющие сэмплировать значения $Z$ из распределения $p(Z)$.
