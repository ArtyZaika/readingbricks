## Вероятностная интерпретация word2vec

Пусть есть выборка пар ($x_i$, $y_{i,c}$), где $i \in \{1, ..., L\}$, $c \in \{1, ..., C\}$, $C$ — как-то заданное количество слов в контексте, $x_i$ — one-hot encoded вектор слова, а $y_{i,c}$ — one-hot encoded вектор слова, встретившегося в контексте слова $x_i$ на позиции $c$.

Допустим, эта выборка порождена из распределения, такого что условные вероятности выхода при условии входа имеют вид $$\mathbb{P}(y_{i,c} \vert x_i) = \frac{\exp(I(x_i)^TO(y_{i,c}))}{\sum_{v \in V}\exp(I(x_i)^TO(v))},$$
где $V$ — множество всех слов, а $I$ и $O$ — некоторые отображения из $\vert V \vert$-мерного в $d$-мерное пространство. Сделаем два наблюдения, касающихся выписанной формулы. Во-первых, в правой части применяется операция softmax. Во-вторых, поскольку все векторы $x_i$ и $y_{i,c}$ таковы, что в них на одной позиции стоит 1, а на всех остальных стоят 0, без ограничения общности можно считать, что $I$ и $O$ задаются умножением на матрицы размера $d \times \vert V \vert$.

Попытаемся найти эти матрицы, решив задачу максимизации правдоподобия имеющейся выборки:
$$\Pi_{i = 1}^L\Pi_{c = 1}^C \mathbb{P}(y_{i,c} \vert x_i) \rightarrow \max_{I, O}.$$

Оказывается, word2vec именно эту задачу и решает. Убедимся в этом.

Неоптимизированная (слишком долго обучающаяся) word2vec принимает на вход one-hot encoded вектор слова, умножает его слева на $d \times \vert V \vert$ матрицу $W_1$ (по сути, это операция взятия соответствующего слову столбца матрицы $W_1$) и делает результат вектором значений скрытого слоя, а этот вектор значений скрытого слоя умножает слева на $\vert V \vert \times d$ матрицу $W_2$, чтобы получить вектор размера $\vert V \vert \times 1$, после применения операции softmax к которому получаются вероятности попадания слов в контекст входного слова. Раз так, то отображением $I$, выучиваемым word2vec, является умножение слева на матрицу $W_1$, а отображением $O$ — умножение слева на транспонированную матрицу $W_2$. Выкладкой проверяется, что задача обучения неоптимизиованной word2vec с кросс-энтропией в качестве функции потерь совпадает с задачей максимизации выписанного правдоподобия.

Небольшой вопрос: почему для слова с идентификатором $j$ выучивается целых два вложения, а именно $j$-й столбец матрицы $W_1$ и $j$-я строка матрицы $W_2$? Дело в том, что положить соответствующие пары весов по определению равными друг другу, чтобы при обучении они учитывались как одна переменная, нельзя, ведь тогда word2vec станет похожей на автокодировщик, и, как следствие, будет завышать вероятность того, что в контексте какого-либо слова встретится оно же само. Это видно и из вероятностной модели — скалярное произведение $I(x_i)^TO(y_{i,c})$ тем больше, чем более коллинеарны множители, а при $I \equiv O$ это создаёт смещение в пользу $x_i$.

Однако обнаруживается, что вышеописанная архитектура word2vec обучается чрезвычайно медленно, так как:

* для вложения $\vert V \vert$ слов в $d$-мерное пространство требуется на каждом шаге обновлять $2 \vert V \vert d$ весов;

* на каждом объекте вычисляется softmax от вектора длины $\vert V \vert$.

Есть два способа ускорить обучение word2vec:

* Noise Contrastive Estimation. В нём применяется отрицательное сэмплирование. Сделать его можно двумя способами:
    - задача $\vert V \vert$-классовой классификации заменяется на задачу бинарной классификации, где от нейронной сети требуется по паре слов сказать, встретились ли они в одном контекстном окне или нет, а для каждого примера положительного класса, взятого из реальных текстов, при том же входном слове сэмплируется несколько выходных слов отрицательного класса, причём сэмплирование производится из эмпирического распределения слов, возведённого в степень 0,75, чтобы редкие слова выбирались чаще.
    - рассматривается задача $\vert V \vert$-классовой классификации, но при обучении считаются константами все веса от входов к скрытому слою кроме весов, исходящих от текущего входного слова, и все веса от скрытого слоя к выходному слою кроме весов, ведущих к текущему выходному слову, и весов, ведущих к скольки-то другим словам отрицательно просэмплированным по процедуре, описанной в предыдущем пункте.

* Hierarchical Softmax. В нём отображение $O$ берётся таким, что областью его определения являются не one-hot encoded векторы слов, а узлы дерева Хаффмана, листьями которого являются слова. В вероятностную модель при этом вносятся соответствующие правки:
$$\mathbb{P}(y_{i,c} \vert x_i) = \Pi_{l \in \mathrm{Path}(y_{i,c})} \sigma\!\left(-\mathrm{Child}(l, y_{i,c}) I(x_i)^TO(l)\right),$$
где $\mathrm{Path}(y_{i,c})$ — путь от корня дерева Хаффмана до листа, соответствующего слову $y_{i,c}$, $\sigma$ — сигмоидная функуция, а $\mathrm{Child}(l, y_{i,c})$ равна 1, если в узле $l$ путь до вершины $y_{i,c}$ поворачивает направо, и -1 иначе.
