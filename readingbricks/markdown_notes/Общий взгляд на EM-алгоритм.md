## Общий взгляд на EM-алгоритм

#### Введение

В задаче кластеризации EM-алгоритм может использоваться для разделения смеси гауссовских распределений. Он имеет ряд преимуществ перед методом K-средних. Во-первых, вместо бинарных индикаторов принадлежности к кластеру вводятся нечёткие степени принадлежности к кластеру. Во-вторых, ковариационные матрицы гауссиан не обязаны быть единичными (как это неявно предполагается при измерении евклидова расстояния до центров кластеров в алгоритме K-средних)— для отсутствия вычислительной неустойчивости достаточно, чтобы они были диагональными, хотя есть и версии с более слабыми ограничениями.

На самом же деле, EM-алгоритм имеет более общий вид, чем алгоритм для разделения смеси гауссовских распределений. Это алгоритм, позволяющий делать две вещи:

* оценивать параметры распределений тогда, когда часть переменных относится к ненаблюдаемым;

* решать задачу статистического вывода (statistical inference), то есть задачу оценивания значений ненаблюдаемых признаков объекта по его значениям наблюдаемых признаков.

Более того, EM-алгоритм может применяться и к выборкам, где векторы объектов не являются независимыми одинаково распределёнными: например, он может применяться к последовательностям (таким как марковские цепи).

#### Постановка задачи

Формализуем постановку задачи следующим образом.

Пусть есть следующие сущности:

* $X$ — $(l \times n)$-матрица наблюдаемых переменных, где $l$ — количество объектов в выборке, а $n$ — количество наблюдаемых признаков;

* $Z$ — $(l \times h)$-матрица скрытых переменных, где $h$ — количество скрытых признаков, а $l$ то же самое, что и в матрице $X$. Для получения полных признаковых описаний объектов выборки матрицу $Z$ нужно соединить с матрицей $X$ боковой конкатенацией;

* $\theta$ — неизвестный вектор параметров вероятностного распределения, породившего выборку; этот вектор требуется оценить по данным; в обычной постановке это детерминированный вектор, в байесовской же постановке (рассматриваемой лишь в самом конце заметки) это случайный вектор со своим собственным распределением.

Про функцию плотности матрицы наблюдаемых переменных пусть будет известно, что она допускает представление в виде:
$$p(X \, \vert \, \theta) = \int p(X, Z \, \vert \, \theta) dZ,$$
где $p(X, Z \, \vert \, \theta)$ дана в аналитическом виде для любой длины $l$ матриц $X$ и $Z$.

Тут уместно пояснить, почему выше используются матрицы $X$ и $Z$ вместо векторов-строк $x$ и $z$, соответствующих какому-либо одному объекту. Дело в том, что различные строки матриц $X$ или $Z$ могут не быть независимыми друг от друга, так что в общем случае функцию плотности нужно брать не от отдельных объектов выборки, а от всей выборки сразу.

Задача заключается в том, чтобы методом максимального правдоподобия найти оценку для $\theta$, имея обучающую выборку $X$, составленную только из наблюдаемых переменных: 
$$\hat{\theta} = \arg\max_\theta \log p(X \, \vert \, \theta).$$
Имея оценку $\hat{\theta}$, можно по матрице с наблюдаемой частью признакового описания объектов $X$ выводить матрицу ненаблюдаемой части признакового описания этих объектов $Z$ следующим образом:
$$\hat{Z} = \arg\max_Z p(X, Z \, \vert \, \hat{\theta}).$$

#### Пример частной постановки

Задача разделения смеси гауссовских распределений является частным случаем описанной задачи, получающимся из неё, если положить, что:

* $X$ — $(l \times n)$-матрица наблюдаемых признаков объектов;

* $Z$ — $(l \times 1)$-матрица (т.е. вектор-столбец) с дискретной скрытой переменной, задающей идентификатор (номер) компоненты смеси, откуда был порождён объект;

* $\theta$ — неизвестный вектор, составленный из:
    - вероятностей компонент смеси $p(z)$, где $z$ — скалярный идентификатор компоненты смеси,
    - центров гауссовских распределений $(\mu_z)_{z=1}^m$, где $m$ — количество компонент смеси,
    - ковариационных матриц гауссовских распределений $(S_z)_{z=1}^m$, развёрнутых из двумерного в одномерный вид, чтобы стать компонентами вектора $\theta$.

Чтобы показать, что задача разделения смеси гауссовских распределений является частным случаем поставленной задачи, необходимо и достаточно убедиться, что $p(X, Z \, \vert \, \theta)$ дана в аналитическом виде. Поскольку в задаче разделения смеси гауссовских распределений объекты являются независимыми одинаково распределёнными, верно, что:
$$p(X, Z  \, \vert \, \theta) = \Pi_{i=1}^l p(x_{i\dot{}}, z_{i\dot{}}  \, \vert \, \theta),$$
где $x_{i\dot{}}$ обозначает $i$-ю строку матрицы $X$, а $z_{i\dot{}}$ обозначает $i$-ю строку матрицы $Z$. Для векторов-строк $x_{i\dot{}}$ и $z_{i\dot{}}$ (т.е. матриц размера $1 \times n$ и $1 \times 1$ соответственно) плотность имеет следующий вид: 
$$p(x_{i\dot{}}, z_{i\dot{}} \, \vert \, \theta) = p(z_{i\dot{}}) \mathcal{N}_{\mu_{z_{i\dot{}}}, S_{z_{i\dot{}}}}(x_{i\dot{}}),$$
где $z_{i\dot{}}$ — как уже упоминалось, скалярный идентификатор компоненты смеси, $\mathcal{N}_{\mu_{z_{i\dot{}}}, S_{z_{i\dot{}}}}(x_{i\dot{}})$ — плотность гауссовского распределения с соответствующими параметрами в точке с координатами $x_{i\dot{}}$, а $p(z_{i\dot{}})$, $\mu_{z_{i\dot{}}}$ и $S_{z_{i\dot{}}}$ извлечены из $\theta$. Подстановкой этого равенства в предыдущее и получается аналитический вид для $p(X, Z  \, \vert \, \theta)$.

#### Обозначения

Обозначим за $q(Z)$ плотность некоторого (не обязательно известного) вероятностного распределения матрицы $Z$. В дальнейшем будет считаться, что эта плотность приближает плотность апостериорной вероятности $p(Z \, \vert \, X, \theta)$. Например, в задаче разделения смеси гауссовских распределений $q(Z)$ хранит информацию о сформировавшихся к текущей итерации EM-алгоритма нечётких степенях принадлежности к кластерам.

Наконец, для определённости в том, использовать ли знак интеграла или знак суммы, будем считать, что $Z$ является матрицей с элементами из $\mathbb{R}$. Если элементы $Z$ дискретны, то интегралы заменятся на суммы, но содержательно ничего не изменится.

#### Приём из вариационного вывода

Для решения общей задачи предпримем следующие шаги, на каждом из которых будем преобразовывать логарифм правдоподобия наблюдаемой выборки:

* $\log p(X \, \vert \, \theta) = \int q(Z) \log p(X \, \vert \, \theta) dZ$, потому что левая часть не зависит от $Z$, а справа её просто внесли как константу в интеграл, равный 1;

* правую часть только что выписанного равенства представим в виде $\int q(Z) \log \frac{p(X, Z \, \vert \, \theta)}{p(Z \, \vert \, X, \theta)} dZ$ — сделать это можно по теореме Байеса;

* домножим выражение под логарифмом на единицу, домножив числитель и знаменатель на одно и то же: $\int q(Z) \log \frac{p(X, Z \, \vert \, \theta) \, q(Z)}{p(Z \, \vert \, X, \theta) \, q(Z)} dZ$;

* воспользуемся тем, что логарифм произведения равен сумме логарифмов, а множители сгруппируем крест-накрест: $\int q(Z) \log \frac{p(X, Z \, \vert \, \theta)}{q(Z)} dZ + \int q(Z) \log \frac{q(Z)}{p(Z \, \vert \, X, \theta)} dZ$.

Первое слагаемое обозначим за $\mathcal{L}(q, \theta)$:
$$\mathcal{L}(q, \theta) = \int q(Z) \log \frac{p(X, Z \, \vert \, \theta)}{q(Z)} dZ.$$
Отметим, что в правой части присутствует $X$, но он не выписан как аргумент, от которого зависит $\mathcal{L}(q, \theta)$. Опустить $X$ можно по следующей причине: это уже заданная константная матрица пронаблюдавшихся значений. Таким образом, $\mathcal{L}(q, \theta)$ является функционалом, зависящим от функции плотности $q$ и вектора $\theta$.

Кроме того заметим, что второе слагаемое в получившемся на последнем шаге выражении равно дивергенции Кульбака-Лейблера между произвольным распределением $q(Z)$ и распределением $p(Z \, \vert \, X, \theta)$, вид которого можно получить из известного по условию распределения $p(X, Z \, \vert \, \theta)$:
$$D_{KL}(q(Z) \, \Vert \, p(Z \, \vert \, X, \theta)) = \int q(Z) \log \frac{q(Z)}{p(Z \, \vert \, X, \theta)} dZ.$$

Итак, путём вышеописанных преобразований было получено, что:
$$\log p(X \, \vert \, \theta) = \mathcal{L}(q, \theta) + D_{KL}(q(Z) \, \Vert \, p(Z \, \vert \, X, \theta)).$$
Второе слагаемое правой части неотрицательно в силу соответствующего свойства дивергенции Кульбака-Лейблера. Из этих соображений первое слагаемое называют нижней вариационной оценкой для $\log p(X \, \vert \, \theta)$.

#### Описание EM-алгоритма

Теперь всё готово, для того чтобы описать EM-алгоритм, пошагово максимизирующий по $\theta$ прологарифмированное правдоподобие $\log p(X \, \vert \, \theta)$:

* E-шаг: при фиксированном $\theta = \theta^{(n)}$ решается задача $\mathcal{L}(q, \theta^{(n)}) \to \max_{q \in Q}$, где $Q$ — некоторое заранее выбранное множество вероятностных распределений. Для получения общего представления о том, как проводится такая максимизация, рассмотрим случай, когда $Q$ — множество всех вероятностных распределений. В этом случае делаются точные E-шаги, а решение оптимизационной задачи получается тогда, когда обнуляется дивергенция Кульбака-Лейблера, поскольку $\mathcal{L}(q, \theta^{(n)})$ не может быть больше $\log p(X \, \vert \, \theta^{(n)})$, в рамках текущего шага являющегося известной константой. Значит, решением задачи максимизации является распределение $q^{(n+1)}(Z) = p(Z \, \vert \, X, \theta^{(n)}) = \frac{p(X, Z \, \vert \, \theta^{(n)})}{\int p(X, Z \, \vert \, \theta^{(n)}) dZ}$, где правая часть (с точностью до взятия интеграла из знаменателя) известна, потому что $X$ известен, $\theta^{(n)}$ известен и по предоположению, сделанному в формулировке задачи, известен аналитический вид $p(X, Z \, \vert \, \theta)$;

* М-шаг: при фиксированном $q = q^{(n)}$ решается задача $\mathcal{L}(q^{(n)}, \theta) \to \max_{\theta}$. Поскольку логарифм частного равен разности логарифмов делимого и делителя, а распределение $q^{(n)}$ зафиксировано, задача эквивалентна задаче $\mathbb{E}_{q^{(n)}} \log p(X, Z \, \vert \, \theta) \to \max_{\theta}$, где матрица $Z$ случайна и именно по её распределению берётся математическое ожидание. Вообще говоря, выписанная оптимизационная задача может быть многоэкстремальной, но если $Q$ — экспоненциальный класс распределений, то задача выпуклая (максимизируется вогнутая функция).

#### Различные версии EM-алгоритма

В зависимости от вида ограничения $q \in Q$ возникают различные версии EM-алгоритма:

* Точный EM-алгоритм. В нём $Q$ — множество всех вероятностных распределений. Этот пример уже разбирался в описании EM-алгоритма. Осталось лишь заметить, что на E-шаге при вычислении решения требуется взять интеграл $\int p(X, Z \, \vert \, \theta^{(n)}) dZ$, но он может не браться. Проблема может возникнуть, если распределения $p(X \, \vert \, Z, \theta^{(n)})$ и $p(Z \, \vert \, \theta^{(n)})$ не образуют пару сопряжённых друг к другу. Поэтому иногда приходится брать как $Q$ нечто более узкое, чем множество всех вероятностных распределений, и проводить приближённые E-шаги.

* Жёсткий (crisp) EM-алгоритм. В нём $Q$ — множество всех атомарных распределений, сконцентрированных в одной точке (в данном случае, в одной матрице): $Q = \{\delta(Z - Z_0) \, \vert \, Z_0 \in \mathbb{R}^{l \times h}\}$, где $\delta$ — дельта-функция Дирака. При таком ограничении E-шаг всегда можно провести: из определения дивергенции Кульбака-Лейблера получится, что решение — это распределение, сконцентрированное в точке моды по $Z$ распределения $p(Z \, \vert \, X, \theta^{(n)}))$. Говоря абстрактно, можно сказать, что в общем случае задача E-шага является задачей функциональной оптимизации (оптимум ищется по вероятностному распределению $q$), а такие задачи чаще всего не могут быть решены средствами современной математики, однако сужением $Q$ до параметрического семейства задачу можно свести к задаче оптимизации по вещественным параметрам (например, все элементы семейства атомарных распределений биективно сопоставляются координатам точки, где сконцентрирована вся масса).

* Вариационный EM-алгоритм. В нём $Q$ — множество распределений, допускающих факторизацию по столбцам: $Q = \{ q(Z)\, \vert \, q(Z) = \Pi_{j=1}^h q_j(z_{\dot{}j})\}$, где $q_j(z_{\dot{}j})$ — некоторое распределение уже только одного вектора-столбца, представляющего $j$-ю скрытую переменную (на самом деле, можно ослабить ограничение, вместо отдельных столбцов перейдя к непересекающимся множествам столбцов). Заметим, что дельта-функция от матрицы раскладывается в произведение дельта-функций от своих столбцов, так что вариационный EM-алгоритм охватывает все варианты, доступные жёсткому EM-алгоритму. На E-шаге решение $q^{(n+1)}$ ищется с использованием вариационного подхода (покоординатного подъёма), и при соблюдении некоторых условий это решение можно найти.

Наконец, отдельно стоит выделить байесовский EM-алгоритм, где $\theta$ рассматривается как случайный вектор, имеющий своё собственное известное априорное распределение $p(\theta)$, а оценить требуется апостериорные распределения $p(\theta \, \vert \, X)$ и $p(Z \, \vert \, X, \theta)$. Введение $p(\theta)$ влечёт за собой то, что вместо $p(X, Z \, \vert \, \theta)$ в выкладках фигурирует $p(X, Z, \theta) = p(X, Z \, \vert \, \theta) p(\theta)$. E-шаг при этом не изменится, потому что $p(\theta^{(n)})$ сократится из выражения для $p(Z \, \vert \, X, \theta^{(n)})$, а вот на M-шаге будет решаться следующая задача:
$$\mathbb{E}_{q^{(n)}} \log p(X, Z, \theta) \to \max_{\theta}.$$
Иными словами, на M-шаге по сравнению с версией без априорного распределения на $\theta$ к оптимизируемому выражению просто добавится слагаемое $\log p(\theta)$.
