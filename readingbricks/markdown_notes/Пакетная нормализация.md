## Пакетная нормализация

Слои нейронов с пакетной нормализацией имеют то преимущество перед обычными слоями, что обучаются быстрее, а ещё им сопутствует некоторая дополнительная регуляризация, благодаря которой порой не нужен дропаут. Правда, обучаться слои с пакетной нормализацией могут только пакетным градиентным спуском.

Везде далее речь пойдёт о нейроне слоя с пакетной нормализацией, а не о целом слое, хотя то же самое можно сформулировать и в терминах слоя.

При пакетной нормализации помимо обычных весов с каждым нейроном ассоциированы следующие дополнительные сущности:

* Два скаляра, обучаемых пакетным градиентных спуском:
    - $\gamma$, скаляр перемасштабирования, инициализируется 1,
    - $\beta$, скаляр сдвига, инициализируется 0;

* Два скаляра, существующих только во время обучения:
    - $\mu_\mathrm{batch}$, среднее по текущему пакету от значений, подаваемых непосредственно в функцию активации нейрона (то есть от значений, возникающих после умножения входов на соответствующие веса, но до взятия функции активации от получившейся суммы),
    - $\sigma_\mathrm{batch}$, среднеквадратичное отклонение значений, подаваемых непосредственно в функцию активации нейрона, подсчитанное по текущему пакету;

* Два скаляра, арифметическими операциями обновляемые на каждом пакете во время обучения и остающиеся константами во время применения, нужны они только на стадии применения:
    - $\mu_\mathrm{cum}$, экспоненциальное среднее от средних по пакетам значений, подаваемых непосредственно в функцию активации нейрона,
    - $\sigma_\mathrm{cum}$, экспоненциальное среднее от внутрипакетных среднеквадратичных отклонений значений, подаваемых непосредственно в функцию активации нейрона.
    
К числу гиперпараметров пакетная нормализация добавляет только коэффициент сглаживания $\alpha$ для подсчёта экспоненциальных средних (или несколько таких коэффициентов для разных слоёв и отдельно для средних и среднеквадратичных отклонений).
   
Идея, стоящая за пакетной нормализацией, восходит к тому, что в машинном обучении бывает полезно центрировать данные и делать их дисперсию единичной. В контексте нейронных сетей это особенно актуально, если функцией активации является сигмоида или гиперболический тангенс, потому что у них градиенты заметно отличаются от нуля лишь в окрестности нуля. Также в случае глубоких нейронных сетей вычитание среднего и деление на дисперсию не могут быть частью предобработки данных, а должны быть частью процесса обучения, потому что до слоёв, расположенных далеко от входных, эффект от разовой предобработки может и не дойти.

Обучение нейрона, с которым на текущий момент ассоциирован вектор-строка весов $w$ и которому в пакете подаются векторы-столбцы $x_i$, где $i$ принимает значения от 1 до размера пакета, выглядит так:

* Вычислить все $u_i = wx_i$ (свободный член отсутствует, потому что его он избыточен в случае пакетной нормализации, ведь его заменяет $\beta$);

* Вычислить $\mu_\mathrm{batch}$ как среднее $u_i$ и вычислить $\sigma_\mathrm{batch}$ как среднеквадратичное отклонение $u_i$;

* Вычислить нормализованные значения $v_i = (u_i - \mu_\mathrm{batch}) \: / \: (\sigma_\mathrm{batch} + \varepsilon)$, где $\varepsilon$ — некоторая малая константа, позволяющая избежать проблем с делением на ноль;

* Обновить значения $\mu_\mathrm{cum}$ и $\sigma_\mathrm{cum}$ по правилу простого экспоненциального сглаживания:
$$\mu_\mathrm{cum} := \alpha\mu_\mathrm{cum} + (1-\alpha)\mu_\mathrm{batch},$$
$$\sigma_\mathrm{cum} := \alpha\sigma_\mathrm{cum} + (1-\alpha)\sigma_\mathrm{batch};$$

* Вычислить значения функции активации $f_\mathrm{activ}(\gamma v_i + \beta)$ и передать каждое из них как вход для следующего слоя на соответствующем ($i$-м) объекте батча;

* При обновлении параметров нейронной сети методом обратного распространения ошибок также считать градиенты по $\gamma$ и $\beta$, чтобы градиентный спуск сам мог решить, какими примерно должны быть средние и дисперсии, ведь это влияет на то, какие участки нелинейности функции активации задействованы.

На стадии применения для нормализации всегда используются $\mu_\mathrm{cum}$ и $\sigma_\mathrm{cum}$, а $\gamma$ и $\beta$ полагаются константами.
