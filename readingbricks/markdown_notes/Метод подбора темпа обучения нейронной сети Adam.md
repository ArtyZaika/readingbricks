## Метод подбора темпа обучения нейронной сети Adam

В классическом варианте градиентного спуска темп обучения один и тот же для всех настраиваемых весов. Исторически одним из первых методов, где это стало не так, был AdaGrad, главный недостаток которого оказался исправлен в методе под названием RMSProp. А метод, в полном виде называющийся Adaptive Moment Estimation и обычно сокращённо называемый Adam, считается дальнейшим развитием адаптивного управления темпом обучения.

Метод Adam имеет четыре гиперпараметра: $\eta$, $\beta_1$, $\beta_2$ и $\varepsilon$. Гиперпараметр $\eta$ задаёт относительную скорость обучения (все адаптивно подобранные темпы обучения содержат его как множитель). Гиперпараметр $\beta_1$ отвечает за сглаживание/угасание накопленного среднего градиентов, а гиперпараметр $\beta_2$ — за сглаживание/угасание накопленного второго момента градиентов, то есть накопленного среднего их квадратов. Наконец, $\varepsilon$ служит для численной стабильности.

Положим, что по аналогии с методом RMSProp формула обновления некого параметра $\theta$ (то есть одного из весов какого-либо слоя нейронной сети) на $t$-м шаге обучения имеет вид:
$$\theta_{t+1} := \theta_t - \frac{\eta}{\sqrt{v_t} + \varepsilon}m_t,$$
где $\theta_t$ — текущее значение параметра $\theta$, $\theta_{t+1}$ — его обновлённое значение, а $m_t$ и $v_t$ — текущие значения присущих алгоритму оптимизации внутренних параметров.

Эти два параметра алгоритма оптимизации $m_t$ и $v_t$ обновляются на каждом шаге по следующим правилам простого экспоненциального сглаживания:
$$m_t = \beta_1 m_{t-1} + (1 - \beta_1) g_{\theta},$$
$$v_t = \beta_2 v_{t-1} + (1 - \beta_2) g^2_{\theta},$$
где $g_{\theta}$ — градиент для параметра нейронной сети $\theta$ на $t$-м шаге обучения. Что касается формулы для $m_t$, экспоненциальное сглаживание в ней можно сравнить с инерцией, но только это уже инерция темпа обучения, а не инерция обновлений весов, часто используемая при обучении нейронных сетей. Что же касается формулы для $v_t$, в ней экспоненциальное сглаживание нужно не для повышения устойчивости, а для того чтобы накопленная сумма квадратов градиентов со временем могла и падать, а не только расти (главный недостаток AdaGrad как раз к тому и сводится, что там она никогда не падает).

Начальными условиями, с которых начинается эволюция во времени параметров алгоритма оптимизации $m_t$ и $v_t$, являются равенства их обоих нулю:
$$m_0 = 0,$$
$$v_0 = 0.$$
Раз так, то значения $m_t$ и $v_t$ при малых $t$ будут смещены в сторону нуля. Чтобы исправить это, вводят следующие скорректированные параметры:
$$\hat{m_t} = \frac{m_t}{1 - \beta_1^t},$$
$$\hat{v_t} = \frac{v_t}{1 - \beta_2^t}.$$

С учётом всего вышесказанного можно выписать окончательную формулу обновления весов методом Adam:
$$\theta_{t+1} := \theta_t - \frac{\eta}{\sqrt{\hat{v_t}} + \varepsilon}\hat{m_t}.$$

Напоследок же стоит заметить, что все методы наподобие AdaGrad, RMSProp или Adam предназначены для ускорения обучения, а не для увеличения качества обученной нейронной сети. В статье [Wilson et al, 2017](https://arxiv.org/abs/1705.08292) показывается, что иногда нейронные сети, обученные ими, проигрывают нейронным сетям, обученным классическим градиентным спуском или стохастическим градиентным спуском. Таким образом, если данных мало, то стоит попробовать не только Adam, но и обычный градиентный спуск.
