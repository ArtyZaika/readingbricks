## Обучение метрик (metric learning)

В машинном обучении есть задача, похожая на задачу классификации, но тем не менее отличная от неё. Пусть множество рассматриваемых объектов — множество пар, элементами которого являются пары $(x_i, x_i^\prime)$, где $x_i$ и $x_i^\prime$ принадлежат одному и тому же пространству, а целевая переменная $y_i$ равна 1, если $x_i$ и $x_i^\prime$ похожи друг на друга, и равна 0, если $x_i$ и $x_i^\prime$ не похожи друг на друга. Требуется выучить отображение $f$, такое что евклидово расстояние между $f(x_i)$ и $f(x_i^\prime)$ мало для похожих объектов и велико для непохожих объектов. Поскольку после решения задачи появляется возможность находить расстояние между объектами, а не только предсказывать, похожи ли они, данная задача отличается от задачи бинарной классификации с признаковым описанием удвоенной длины.

Обучение метрик может быть полезно для решения задачи многоклассовой классификации с высоким числом классов, таких что каждый класс представлен малым числом объектов обучающей выборки. Положим все объекты одного и того же класса похожими друг на друга, а объекты разных классов отличающимися. Предсказывать классы новых объектов можно, применяя метод ближайшего соседа в пространстве, получающемся после применения отображения $f$.

Для обучения метрик иногда используют функцию потерь, называемую contrastive loss:
$$l(x_i, x_i^\prime, f, y_i) = y_i\Vert f(x_i) - f(x_i^\prime)\Vert_2 + (1 - y_i)\max(1 - \Vert f(x_i) - f(x_i^\prime)\Vert_2, 0).$$
Эта функция потерь состоит из двух слагаемых. Первое отлично от нуля, когда взята пара похожих объектов, и это слагаемое побуждает минимизировать евклидово расстояние между тем, во что переходят похожие объекты. Второе слагаемое отлично от нуля, когда взята пара непохожих объектов, и это слагаемое побуждает отображать непохожие объекты так, чтобы расстояние между образами под действием $f$ было больше некоторого фиксированного порога.

Эмпирический риск для задачи обучения с contrastive loss, как и следовало ожидать, имеет вид:
$$E(f) = \frac{1}{n}\sum_{i = 1}^n  l(x_i, x_i^\prime, f, y_i) + \alpha R(f),$$
где через регуляризатор $R$ и силу регуляризации $\alpha$ можно наложить дополнительные ограничения.

Если считать, что отображение $f$ задаётся умножением слева на матрицу $M$ размера $k \times n$, то вышеуказанная функция потерь будет плоха тем, что задача её оптимизации по $M$ не является выпуклой из-за отрицательного знака перед вторым слагаемым. В таком случае берут немного другую функцию потерь, отталкивающуюся от того, что если обозначить матрицу $M^TM$ размера $n \times n$ за $S$, то $\Vert Mx_i - Mx_i^\prime\Vert_2 = (x_i - x_i^\prime)^T S (x_i - x_i^\prime)$:
$$l(x_i, x_i^\prime, S, y_i) = y_i(x_i - x_i^\prime)^T S (x_i - x_i^\prime) + (1 - y_i)\max(1 - (x_i - x_i^\prime)^T S (x_i - x_i^\prime), 0).$$
При минимизации эмпирического риска по матрице $S$ размера $n \times n$ стоит учитывать ограничение, что $S$ должна быть неотрицательно определённой, иначе её нельзя было бы представить в виде $S = M^TM$. Данное ограничение является выпуклым. Однако платой за превращение задачи оптимизации в выпуклую является то, что нельзя заранее задать размерность выходного пространства, ведь ограничения на ранг матрицы $S$ не являются выпуклыми.
