## На пути к word2vec

Помимо word2vec есть некоторые более простые способы отобразить слово в непрерывный вектор.

#### Вложение на базе разбивки по коллекциям слов

* Построим матрицу, где строкам соответствуют слова, столбцам — некоторые коллекции слов (например, документы или предложения), а на пересечении строки и столбца может стоять:
    - бинарный индикатор вхождения слова в коллекцию;
    - количество вхождений слова в коллекцию;
    - TF-IDF;
    - BM-25.

* Применим к получившейся матрице какую-либо процедуру снижения размерности, такую как:
    - метод главных компонент, PCA;
    - многомерное шкалирование, MDS;
    - вложение на базе случайного соседства, t-SNE.

* Примем за векторное представление некоторого слова соответствующую этому слову строку матрицы со сниженной размерностью.

#### Вложение через переходные вероятности

* Пронумеруем все слова.

* Возьмём матрицу размера $\vert V \vert \times \vert V \vert$, где $\vert V \vert$ — количество слов, на пересечении $i$-й строки и $j$-го столбца которой стоит вероятность того, что сразу же после $i$-го слова в тексте встретится $j$-е слово. Казалось бы, вложением $i$-го слова можно считать $i$-ю строку такой матрицы или $i$-й столбец такой матрицы, однако у столь простого подхода есть два недостатка:
    - Большое количество оцениваемых по данным параметров, а именно $\vert V \vert^2$ элементов матрицы переходных вероятностей;
    - Размерность вектора вложения такая же, как у вектора, получаемого через one-hot encoding;
    - Подобное представление не очень информативно, хотя и более информативно, чем представление, получаемое через one-hot encoding.

* Попробуем получить более разумное вложение за счёт большей компактности представления. Предположим, что для некоторого фиксированного $d < \vert V \vert$ откуда-то даны две матрицы $W_1$ и $W_2$ размеров $d \times \vert V \vert$ и $\vert V \vert \times d$ соответственно, такие что матрицу переходных вероятностей удалось приблизить матрицей $W_2 W_1$. Последнее предположение формализуется и уточняется так: $\forall i, i \in \{1, ..., \vert V \vert\}$, после применения операции softmax к $\vert V \vert \times 1$ вектору-столбцу $W_2 W_1 x_i$, где $x_i$ — вектор-столбец размера $\vert V \vert \times 1$, такой что его $i$-я компонента равна 1, а остальные равны 0 (т.е. $x_i$ — one-hot представление $i$-го слова), получается вектор, близкий к вектору переходных вероятностей из $i$-го слова. В таком случае вложением слова в векторное пространство можно считать что угодно из этого списка:
    - $i$-й столбец матрицы $W_1$;
    - $i$-ю строку матрицы $W_2$;
    - их полусумму (строго говоря, строку или столбец для этого нужно транспонировать);
    - их конкатенацию (опять же, строку или столбец необходимо транспонировать).

* Описанный в предыдущем пункте подход имеет следующие преимущества:
    - Всего $2 d \vert V \vert$ обучаемых параметров;
    - Размерность вектора вложения можно сделать маленькой;
    - Получаются более информативные векторные представления, поскольку теперь $\vert V \vert \times \vert V \vert$ матрица переходных вероятностей не может быть произвольной, а должна быть сводимой к виду $W_2 W_1$ с точностью до применения операции softmax к каждому столбцу указанного произведения двух матриц.

#### Связь word2vec с вложением через переходные вероятности

Нейронная сеть word2vec является способом найти описанные в предыдущем разделе матрицы $W_1$ и $W_2$, и в этом способе в связи с особенностями обучения нейронных сетей возникают различные специальные трюки наподобие отрицательного сэмплирования. Главное отличие в постановке задачи машинного обучения заключается в том, что в word2vec выходное слово уже не обязано идти сразу же после входного слова, а должно лишь попасть в контекст определённой ширины.  

Впрочем, иногда считают, что также в word2vec вводится зависимость структуры нейронной сети от выбранного размера контекстного окна. Обозначим размер контекстного окна за $C$. Тогда:

* в word2vec-CBoW вход имеет длину $C \vert V \vert$ и в скрытом слое усредняются $C$ произведений одной и той же матрицы $W_1$ со входными one-hot векторами, каждый из которых имеет длину $\vert V \vert$;

* в word2vec-SkipGram выход имеет длину $C \vert V \vert$ и для этих $C$ блоков, представляющих вероятности попадания слова в какую-либо из позиций контекста входного слова, происходит умножение одной и той же матрицы $W_2$ на вектор скрытого слоя (то, что во всех $C$ копиях будет одно и то же вероятностное распределение, нестрашно, ведь задача не в том, чтобы предсказать контекст, а в том, чтобы обучить вложение).

Так или иначе, есть и подход к word2vec, где вход и выход имеют длину $\vert V \vert$, то есть размеры слоёв не зависят от размера контекстного окна.
