{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "временные_ряды"
    ]
   },
   "source": [
    "## Прогнозирование на несколько шагов вперёд ансамблями над регрессионными деревьями\n",
    "\n",
    "Регрессионные деревья и ансамбли над ними по умолчанию не предназначены для учёта временной структуры. Однако прогнозирование на несколько разных горизонтов одной и той же моделью, базирующейся на деревьях, можно осуществить следующим образом.\n",
    "\n",
    "Возьмём обучающую выборку для задачи прогнозирования на один шаг вперёд. При группировке по идентификатору объекта применим операцию сдвига целевой переменной на $k$ шагов, где $k$ принимает значения от 0 до $(h-1)$, а $h$ — желаемый горизонт прогнозирования. Получим выборку, которая в $h$ раз больше по размеру, чем исходная. Добавим в эту выборку признак, принимающий значения от 1 до $h$ и равный $(k+1)$. Этот признак можно интерпретировать как то, на сколько шагов вперёд строится прогноз. Если теперь обучить модель на таких данных, то в зависимости от поданного на стадии предсказания значения нового признака модель сможет предсказывать на любое количество шагов вперёд от 1 до $h$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "нейронные_сети"
    ]
   },
   "source": [
    "## Многозадачное обучение (multi-task learning)\n",
    "\n",
    "Допустим, есть вспомогательная задача, для которой имеется много размеченных данных, и есть основная задача, для которой размеченных данных существенно меньше. Также допустим, что эти две задачи работают с одними и теми же объектами, но предсказывают на них разные целевые функции. Например, во вспомогательной задаче нужно определить пол человека с портретного фото, а в основной задаче требуется определить цвет волос.\n",
    "\n",
    "В описанной ситуации можно поступить следующим образом. Построим нейронную сеть для решения вспомогательной задачи и обучим её. Затем уберём сколько-то последних слоёв получившейся сети, и вместо них поставим новые, такие что размерность выходного слоя уже соответствует основной задаче.\n",
    "\n",
    "Далее есть два варианта:\n",
    "\n",
    "1) Зафиксировать веса оставленных слоёв нейронной сети, обученной под вспомогательную задачу, и просто прогонять объекты через оставленные слои как через статичный предобработчик;\n",
    "\n",
    "2) Веса оставшихся от вспомогательной задачи слоёв обучать наравне с весами новых слоёв, то есть считать, что они просто были инициализированы по-другому, но больше отличий между ними и весами новых слоёв нет.\n",
    "\n",
    "Какой из вариантов выбрать, зависит от числа данных в основной задаче. Если острого недостатка в них нет, то второй способ, как правило, даёт более высокий результат. Первый способ, однако, быстрее, и также он менее требователен к количеству данных (а если размеченных данных совсем мало, то можно даже использовать результат прогона через оставленные обученные слои в качестве признаков для более простого метода: например, метода ближайших соседей).\n",
    "\n",
    "Новая нейронная сеть обучится лучше, чем если бы сеть с такой же архитектурой обучалась на данных для основной задачи с нуля. Дело в том, что при описанном подходе нейронная сеть будет опираться на скрытое представление, выученное по большому количеству размеченных данных оставленными слоями старой нейронной сети. Также подстройка весов под разные задачи способствует увеличению обобщающей способности. Подробности о том, почему многозадачное обучение работает, есть в разделе 3 статьи [Ruder, 2017](https://arxiv.org/pdf/1706.05098.pdf).\n",
    "\n",
    "Ещё один подход к многозадачному обучению таков: обучать нейронную сеть параллельно то под вспомогательную задачу, то под основную, при каждом переключении задачи оставляя сколько-то первых слоёв, но заменяя последние слои на то, что было ранее в задаче, к которой происходит переход."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "выбор_модели"
    ]
   },
   "source": [
    "## Как подобрать сложность модели без кросс-валидации?\n",
    "\n",
    "Иногда в моделях из одного и того же семейства есть параметр, регулирующий сложность. Например, в семействе полиномиальных функций сложностью можно считать максимальную допустимую степень многочлена. Довольно часто сложность можно положить равной количеству оцениваемых по данным параметров — эта формулировка, в частности, включает в себя предыдущую, ведь при оценивании многочленом степени не выше $d$ оценивается $d+1$ параметр.\n",
    "\n",
    "Модели из одного и того же семейства, отличающиеся друг от друга только сложностью, можно оценивать при помощи информационных критериев.\n",
    "\n",
    "Информационный критерий Акаике имеет вид:\n",
    "$$\\mathrm{AIC} = 2k - 2l,$$\n",
    "где $k$ — количество оценённых по данным параметров, а $l$ — значение функционала качества, достигнутое на тренировочной выборке (например, логарифм правдоподобия данных при условии того, что выборка порождается из распределения, задаваемого той моделью, которая была построена на тренировочной выборке).\n",
    "\n",
    "Байесовский информационный критерий имеет вид:\n",
    "$$\\mathrm{BIC} = \\log{(n)} \\, k - 2l,$$\n",
    "где $n$ — размер тренировочной выборки.\n",
    "\n",
    "По сути, информационные критерии представляют собой оценку качества подгонки под данные с дополнительным штрафом за сложность модели. Интуитивно говоря, чем сложнее модель, тем больше ситуаций, под которые её можно подогнать и, значит, тем менее ценны уровни качества подгонки под данные.\n",
    "\n",
    "Чем ниже значение какого-либо информационного критерия, тем более хорошей считается модель."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "бизнес"
    ]
   },
   "source": [
    "## Откуда может взяться недовольство заказчика машинно-обученной моделью?\n",
    "\n",
    "Любая прикладная задача моделирования имеет заказчика, неважно, внутреннего или внешнего. Когда заказчик принимает модель, он решает, устраивает ли она его. Причины недовольства заказчика бывают такие:\n",
    "\n",
    "1) Модель, и в самом деле, плохая;\n",
    "\n",
    "2) Заказчик не умеет пользоваться моделью;\n",
    "\n",
    "3) Ожидания заказчика неоправданно завышены.\n",
    "\n",
    "Какой бы ни была истинная причина, заказчик, скорее всего, решит, что реализовался первый сценарий: модель плохая. Поэтому в интересах специалиста по моделированию избежать ситуаций, когда хорошая модель попадает во второй или третий сценарий.\n",
    "\n",
    "Разберём эти два сценария подробнее.\n",
    "\n",
    "Пример сценария, когда моделью не умеют пользоваться, таков: сервис на базе модели рассылает своевременные уведомления о появлении объектов положительного класса, но люди, которым они приходят, игнорируют их. Говоря более общо, можно сказать, что возможны осложнения с интеграцией в бизнес-процессы заказчика. Чтобы упредить их возникновение, полезно задать следующие вопросы:\n",
    "* Что должно происходить с результатами работы сервиса?\n",
    "* Как именно сервис приносит конечную ценность?\n",
    "* Все ли исполнители знают и понимают ответы на два предыдущих вопроса?\n",
    "* Можно ли оперативно отслеживать сбои в исполнении действий на базе сервиса и можно ли предложить количественные метрики оценки качества исполнения таких действий?\n",
    "* Все ли признаки, использованные при обучении модели, будут доступны на стадии применения модели вовремя и в сопоставимом качестве?\n",
    "\n",
    "Проблему завышенных ожиданий проще всего решить, ещё до старта работ по проекту согласовав метрику успеха и её целевой уровень. Сделать это можно, проведя оценку конечного эффекта от внедрения сервиса. Этот конечный эффект может быть как финансовым, то есть выражающимся в деньгах, так и каким-либо более широким (скажем, социальным), но, так или иначе, измеримым.\n",
    "\n",
    "Впрочем, бывают ситуации, когда заказчик рассуждает не в терминах конечного эффекта, а в терминах всестороннего описания зависимостей — в этом случае заказчик может потребовать, скажем, почти стопроцентную точность по положительному классу. Разумеется, модель, не способная точно описать все зависимости, тоже может быть полезна. Проиллюстрировать это заказчику можно при помощи такого сравнения: если есть лотерея, где вероятность выиграть равна $p$, стоимость участия составляет $c$, а выигрыш приносит $g$, то ответ на вопрос, рационально ли участвовать в такой лотерее, зависит не от того, насколько вероятность $p$ близка к 100%, а от того, как друг с другом соотносятся все три указанных параметра, ведь ожидаемый чистый выигрыш равен $pg - c$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "нейронные_сети"
    ]
   },
   "source": [
    "## Субдискретизация (pooling)\n",
    "\n",
    "Субдискретизация (pooling) устроена так: по входу слоя проходит фильтр (filter, kernel) и каждый раз от всего, что попадает в него, берёт значение некоторой агрегирующей функции, становящееся значением в соответствующем узле (нейроне) (unit, neuron). Например, в качестве агрегирующей функции можно взять максимум — тогда речь пойдёт о субдискретизации максимумом (max pooling).\n",
    "\n",
    "Какие желаемые свойства добавляет субдискретизация максимумом?\n",
    "* Возникает устойчивость к сдвигам изображения на небольшое количество пикселей;\n",
    "* Появляется возможность реализовать следующую логику: присутствие детектируемого объекта на объединении участков зависит от того, присутствует ли объект хотя бы на одном из этих участков, а не от доли участков, на которых он присутствует."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "общая_теория"
    ]
   },
   "source": [
    "## Три фундаментальных источника ошибок в машинном обучении\n",
    "\n",
    "Оптимальный в байесовском смысле регрессор (или классификатор) — это то, что задаётся следующим формальным выражением:\n",
    "$$f_{opt} = \\arg \\min_{f \\in F} \\int_{(x, y) \\sim P_{true}(x, y)}l(f(x), y),$$\n",
    "где $F$ — множество всех возможных регрессоров (или классификаторов соответственно), $P_{true}$ — истинное совместное распределение признаков и целевой переменной, а $l$ — функция потерь, по значению регрессора (классификатора) на векторе признаков и целевой переменной на этом векторе признаков возвращающая соответствующий штраф.\n",
    "\n",
    "В реальности есть ограничения, не позволяющие найти $f_{opt}$.\n",
    "\n",
    "Во-первых, неизбежна ошибка приближения (approximation error): поиск проводится не по всему множеству возможных регрессоров (классификаторов), а лишь по какому-то подмножеству $\\overline{F} \\subset F$, например, по некому параметрическому семейству. Скажем, в случае с линейной регрессией $\\overline{F}$ — это множество всех линейных регрессоров. Как следствие, ищется уже нечто другое:\n",
    "$$\\hat{f_1} = \\arg \\min_{f \\in \\overline{F}} \\int_{(x, y) \\sim P_{true}(x, y)}l(f(x), y).$$\n",
    "Ошибка, вызываемая отличием $\\hat{f_1}$ от $f_{opt}$, и называется ошибкой приближения.\n",
    "\n",
    "Во-вторых, существует ошибка оценивания по данным (estimation error): невозможно получить истинную генеральную совокупность $P_{true}$, а вместо неё есть лишь выборка конечного размера $N$. С учётом такого ограничения задача принимает вид:\n",
    "$$\\hat{f_2} = \\arg \\min_{f \\in \\overline{F}} \\frac{1}{N} \\sum_{i=1}^{N} l(f(x_i), y_i) + \\alpha R(f),$$\n",
    "где $(x_i, y_i)$ — признаки и ответ на $i$-м объекте обучающей выборки, $R(f)$ — регуляризатор, а $\\alpha$ — коэффициент, задающий силу регуляризации. Ошибка оценивания — ошибка, вызываемая отличием $\\hat{f_2}$ от $\\hat{f_1}$.\n",
    "\n",
    "В-третьих, не всегда удаётся довести поиск минимума из предыдущего пункта до конца, и отсюда возникает ошибка оптимизации (optimization error). Если предположить, что вычислительные ресурсы ограничены и есть всего лишь $T$ итераций, то получится, что решается задача\n",
    "$$\\hat{f_3}^{(T)} = \\arg \\min_{(T); f \\in \\overline{F}} \\frac{1}{N} \\sum_{i=1}^{N} l(f(x_i), y_i) + \\alpha R(f),$$\n",
    "где запись $(T)$ под минимумом обозначает, что минимум ищется за ограниченное число шагов. По аналогии с определением предыдущих ошибок ошибка оптимизации — это ошибка, вызываемая отличием $\\hat{f_3}^{(T)}$ от $\\hat{f_2}$.\n",
    "\n",
    "Можно считать, что машинное обучение на больших данных отличается от машинного обучения на маленьких данных тем, что ошибка оптимизации начинает доминировать над ошибкой оценивания по данным.\n",
    "\n",
    "Подробности есть в статье [Bottou, Bousquet, 2007](https://papers.nips.cc/paper/3323-the-tradeoffs-of-large-scale-learning.pdf)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "подготовка_данных"
    ]
   },
   "source": [
    "## Способы автоматического обнаружения выбросов\n",
    "\n",
    "Выброс (outlier) — объект, считающийся нетипичным для генеральной совокупности, фигурирующей в задаче. Выбросы могут искажать результаты обучения модели, а также результаты оценки качества модели (что создаёт риск неправильного подбора гиперпараметров). Строго говоря, является ли какой-то объект выбросом или нет, должен определять человек, отталкиваясь от своего понимания предметной области и от знаний о том, как именно собирались данные. Тем не менее на практике автоматическое удаление выбросов тоже может существенно улучшить конечный результат.\n",
    "\n",
    "Способы обнаружения выбросов делятся на две группы в зависимости от того, на какие выбросы они реагируют:\n",
    "\n",
    "1) Способы, позволяющие выявить одномерные выбросы, то есть выбросы, являющиеся аномальными значениями какого-либо одного признака. Пример одномерного выброса: среди точек на плоскости с координатами (1, 1), (2, 2), (3, 3), (4, 4), (5, 5), (100, 6) у последней точки подозрительно большая абсцисса.\n",
    "\n",
    "2) Способы, позволяющие выявить многомерные выбросы, то есть выбросы, являющиеся аномальными объектами с точки зрения распределения данных. Пример многомерного выброса: среди точек на плоскости с координатами (1, 1), (2, 2), (3, 3), (4, 4), (5, 5), (1, 5) последняя точка лежит в стороне от прямой, на которой лежат все остальные точки, но при этом и её абсцисса, и её ордината не выбиваются из диапазона значений остальных точек.\n",
    "\n",
    "К одномерным способам относятся:\n",
    "* Стандартизированная оценка (Z-score) — из всех значений признака вычитается эмпирическое среднее этого признака, а полученные разности делятся на оценку стандартного отклонения признака; если получившееся отношение превышает 3, то объект считается выбросом. Порог 3 выбран, потому что в случае нормального распределения вероятность отклониться от среднего больше, чем на 3 стандартных отклонения, составляет 0,03% ([правило трёх сигм](https://ru.wikipedia.org/wiki/%D0%A1%D1%80%D0%B5%D0%B4%D0%BD%D0%B5%D0%BA%D0%B2%D0%B0%D0%B4%D1%80%D0%B0%D1%82%D0%B8%D1%87%D0%B5%D1%81%D0%BA%D0%BE%D0%B5_%D0%BE%D1%82%D0%BA%D0%BB%D0%BE%D0%BD%D0%B5%D0%BD%D0%B8%D0%B5#%D0%9F%D1%80%D0%B0%D0%B2%D0%B8%D0%BB%D0%BE_%D1%82%D1%80%D1%91%D1%85_%D1%81%D0%B8%D0%B3%D0%BC)). \n",
    "* Устойчивая стандартизированная оценка (robust Z-score) — то же самое, что и стандартизированная оценка, но теперь вычитается медиана, а не среднее, и разности делятся на средний модуль отклонения от медианы, а не на стандартное отклонение. Предпочтительнее в тех ситуациях, где выбросов достаточно много для того, чтобы они могли исказить среднее и стандартное отклонение так, что часть из них будет принята за обычные объекты.\n",
    "* Метод межквартильного размаха (interquartile range method) — берётся расстояние между 0,75-й и 0,25-й квантилями (так называемый межквартильный размах), а затем выбросами объявляются все наблюдения, отстоящие от медианы более чем на 1,5 межквартильных размаха. Численные параметры приведены в соответствии с классическим вариантом, но, разумеется, их можно поменять (например, вместо двух квартилей взять две другие квантили).\n",
    "\n",
    "К многомерным способам относятся:\n",
    "* Способы, где для каждого объекта считается расстояние Махалонобиса от него до некоторого оценённого по данным распределения, принимаемого за генеральную совокупность, а потом некой одномерной процедурой выбросами объявляются сколько-то объектов с наибольшим расстоянием Махалонобиса. Рассмотрим такой подход подробнее. Для многомерного вектора $x$ и многомерного вероятностного распределения $\\tau(\\mu, S)$ с вектором средних $\\mu$ и ковариационной матрицей $S$ \"расстоянием\" от $x$ до $\\tau(\\mu, S)$ можно положить величину:\n",
    "$$D_M(x, \\tau(\\mu, S)) = \\sqrt{(x - \\mu)^T S^{-1} (x - \\mu)},$$\n",
    "которая и называется расстоянием Махалонобиса. Интуитивно говоря, расстояние Махалонобиса отличается от евклидова расстояния между $x$ и $\\mu$ тем, что каждое направление учитывается с весом, обратно пропорциональным разбросу распределения по этому направлению. В задаче обнаружения выбросов $\\tau(\\mu, S))$ можно единожды оценить по всем имеющимся данным, если их много, или же для каждого $x$ оценивать по всем объектам кроме $x$, если данных мало.\n",
    "* Способы, где для каждого объекта считается оценка плотности генеральной совокупности в нём, а затем некой одномерной процедурой выбросами объявляются сколько-то объектов с наиболее низкой оценкой плотности. Оценку плотности можно получить, в частности, методом ядреного сглаживания (kernel density estimate). Как и в предыдущем пункте, если данных мало, то оценку ядерного сглаживания для каждого объекта лучше считать по всем объектам кроме него, а если данных много, то оценку ядерного сглаживания можно подсчитать один раз на всех объектах. Отдельно стоит отметить, что оценки ядерного сглаживания подвержены \"проклятью размерности\" и работают только в случае, когда размерность данных низкая. Если же размерность высокая, то для применения ядерного сглаживания можно предварительно снизить её, использовав PCA, t-SNE или MDS на отмасштабированных данных, однако как проводить масштабирование — нетривиальный вопрос, вносящий долю субъективности. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "нейронные_сети"
    ]
   },
   "source": [
    "## Автокодировщики с большей размерностью скрытого слоя\n",
    "\n",
    "Рассмотрим автокодировщик (autoencoder) со входным слоем размерности $l$, одним скрытым слоем размерности $h$ и выходным слоем, размерность которого по определению должна быть равна размерности входного слоя, то есть $l$. Вопрос: могут ли для чего-то пригодиться автокодировщики, где $h > l$?\n",
    "\n",
    "Без дополнительных модификаций такие автокодировщики не смогут обучаться чему-то нетривиальному, потому что всегда есть вырожденное решение: использовать лишь $l$ нейронов из $h$, чтобы просто передать вход в неизменном виде.\n",
    "\n",
    "Есть следующие способы заставить скрытый слой выучивать нетривиальное представление:\n",
    "* К ошибке восстановления, на входном векторе $x$ равной $\\Vert x - \\mathrm{autoencoder}(x)\\Vert_2$ добавить регуляризатор, подталкивающий к разреженности скрытого слоя: например, $L_1$-норму $h$-мерного вектора, получающегося из $x$ в скрытом слое;\n",
    "* Ко входу подмешивать шум, но при вычислении ошибки восстановления требовать близости к незашумлённому входу, то есть минимизировать математическое ожидание функции, на входном векторе $x$ равной $\\Vert x - \\mathrm{autoencoder}(x + \\varepsilon)\\Vert_2$, где $\\varepsilon$ — шум.\n",
    "* Каждую из компонент вектора входов с некоторой вероятностью $p$ обнулять, но при вычислении ошибки восстановления сверяться с вектором входов, где все компоненты имеют исходный вид — так автокодировщик будет побуждён выучивать взаимосвязи между признаками, позволяющие по части признаков восстановить остальные признаки. \n",
    "\n",
    "Полученный автокодировщик можно использовать в следующих целях:\n",
    "* То, что получается в скрытом слое, можно брать в качестве признакового описания объекта; такие признаки могут содержать в себе более сложные взаимодействия, чем признаки, даваемые автокодировщиком с низким $h$; пригождается это в частичном обучении (semi-supervised learning), ведь объекты без меток тоже влияют на выучиваемый способ порождать признаки;\n",
    "* До появлении пакетной нормализации и остаточного обучения была популярна \"жадная\" (послойная от входов к выходам) инициализация весов глубокой однонаправленной нейронной сети, при которой инициализируемый на данный момент слой дополнялся до автокодировщика и обучался задаче автокодировщика, где кодировалось то, что подавал на вход предыдущий слой, — стало быть, если в каком-то месте ширина нейронной сети возрастала, то требовалось обучать автокодировщик, у которого $h > l$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "постановка_задачи"
    ]
   },
   "source": [
    "## Обучение метрик (metric learning)\n",
    "\n",
    "В машинном обучении есть задача, похожая на задачу классификации, но тем не менее отличная от неё. Пусть множество рассматриваемых объектов — множество пар, элементами которого являются пары $(x_i, x_i^\\prime)$, где $x_i$ и $x_i^\\prime$ принадлежат одному и тому же пространству, а целевая переменная $y_i$ равна 1, если $x_i$ и $x_i^\\prime$ похожи друг на друга, и равна 0, если $x_i$ и $x_i^\\prime$ не похожи друг на друга. Требуется выучить отображение $f$, такое что евклидово расстояние между $f(x_i)$ и $f(x_i^\\prime)$ мало для похожих объектов и велико для непохожих объектов. Поскольку после решения задачи появляется возможность находить расстояние между объектами, а не только предсказывать, похожи ли они, данная задача отличается от задачи бинарной классификации с признаковым описанием удвоенной длины.\n",
    "\n",
    "Обучение метрик может быть полезно для решения задачи многоклассовой классификации с высоким числом классов, таких что каждый класс представлен малым числом объектов обучающей выборки. Положим все объекты одного и того же класса похожими друг на друга, а объекты разных классов отличающимися. Предсказывать классы новых объектов можно, применяя метод ближайшего соседа в пространстве, получающемся после применения отображения $f$.\n",
    "\n",
    "Для обучения метрик иногда используют функцию потерь, называемую contrastive loss:\n",
    "$$l(x_i, x_i^\\prime, f, y_i) = y_i\\Vert f(x_i) - f(x_i^\\prime)\\Vert_2 + (1 - y_i)\\max(1 - \\Vert f(x_i) - f(x_i^\\prime)\\Vert_2, 0).$$\n",
    "Эта функция потерь состоит из двух слагаемых. Первое отлично от нуля, когда взята пара похожих объектов, и это слагаемое побуждает минимизировать евклидово расстояние между тем, во что переходят похожие объекты. Второе слагаемое отлично от нуля, когда взята пара непохожих объектов, и это слагаемое побуждает отображать непохожие объекты так, чтобы расстояние между образами под действием $f$ было больше некоторого фиксированного порога.\n",
    "\n",
    "Эмпирический риск для задачи обучения с contrastive loss, как и следовало ожидать, имеет вид:\n",
    "$$E(f) = \\frac{1}{n}\\sum_{i = 1}^n  l(x_i, x_i^\\prime, f, y_i) + \\alpha R(f),$$\n",
    "где через регуляризатор $R$ и силу регуляризации $\\alpha$ можно наложить дополнительные ограничения.\n",
    "\n",
    "Если считать, что отображение $f$ задаётся умножением слева на матрицу $M$ размера $k \\times n$, то вышеуказанная функция потерь будет плоха тем, что задача её оптимизации по $M$ не является выпуклой из-за отрицательного знака перед вторым слагаемым. В таком случае берут немного другую функцию потерь, отталкивающуюся от того, что если обозначить матрицу $M^TM$ размера $n \\times n$ за $S$, то $\\Vert Mx_i - Mx_i^\\prime\\Vert_2 = (x_i - x_i^\\prime)^T S (x_i - x_i^\\prime)$:\n",
    "$$l(x_i, x_i^\\prime, S, y_i) = y_i(x_i - x_i^\\prime)^T S (x_i - x_i^\\prime) + (1 - y_i)\\max(1 - (x_i - x_i^\\prime)^T S (x_i - x_i^\\prime), 0).$$\n",
    "При минимизации эмпирического риска по матрице $S$ размера $n \\times n$ стоит учитывать ограничение, что $S$ должна быть неотрицательно определённой, иначе её нельзя было бы представить в виде $S = M^TM$. Данное ограничение является выпуклым. Однако платой за превращение задачи оптимизации в выпуклую является то, что нельзя заранее задать размерность выходного пространства, ведь ограничения на ранг матрицы $S$ не являются выпуклыми."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "постановка_задачи"
    ]
   },
   "source": [
    "## Искусственная целевая переменная\n",
    "\n",
    "Бывают ситуации, когда достоверно целевая переменная неизвестна, но её можно восстановить оценочно. Например, в задаче обнаружения подозрительного трафика, приходящего на сайт, каких-то пользоваталей можно отнести к ботам или к живым людям на основании экспертного мнения.\n",
    "\n",
    "Рассмотрим две ситуации:\n",
    "* Специалисты по предметной области написали детерминированную программу, на базе свода правил и формул вычисляющую целевую переменную по входным признакам;\n",
    "* Специалисты по предметной области не смогли составить список правил и формул, по которым можно вычислять целевую переменную, но внимательно изучили данные и разметили каждый пример вручную.\n",
    "\n",
    "Вопрос: в каких из этих двух ситуаций стоит применять машинное обучение?\n",
    "\n",
    "Ответ: только во второй. В первой ситуации построенная модель будет вести себя точно так же, как уже написанная программа, с точностью до ошибок, вызываемых нерепрезентативностью обучающей выборки, малым количеством данных и/или артефактами обучения. Иными словами, нет ничего, что позволило бы модели стать лучше, чем программа. А вот во второй ситуации модель, если хорошо обучится, станет автоматическим воплощением интуиции и знаний экспертов.\n",
    "\n",
    "Однако если немного изменить условие первой ситуации, то и в ней машинное обучение может помочь. Например, пусть программа в своих вычислениях использует также некоторый дополнительный внешний признак, такой что измерять его дорого, и который поэтому не будет доступен на стадии ежедневного применения обученной модели. Тогда можно рассчитать целевую переменную по всем признакам включая новый трудноизмеримый, а потом обучить модель предсказывать полученную целевую переменную только по признакам, которые всегда будут доступны."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "постановка_задачи"
    ]
   },
   "source": [
    "## Классификация с очень большим количеством классов\n",
    "\n",
    "Пусть есть задача классификации, где различных меток много в сравнении с количеством объектов обучающей выборки (например, меток лишь в три раза меньше, чем объектов). В таком случае задача осложняется тем, что модель будет довольно редко угадывать правильную метку, и, значит, оптимизировать функции потерь, связанные с точными попаданиями, непрактично.\n",
    "\n",
    "Предположим, что задача классификации с $n$ признаками и с $q$ классами, где $q$ большое в вышеописанном смысле, решается путем поиска матрицы $W$ размера $n \\times q$, такой что близость объекта, признаковое описание которого задаётся вектором-столбцом $x$ длины $n$, к классу, закодированному вектором-столбцом $y$ длины $q$, где на позиции, номер которой равен номеру класса, стоит 1, а на остальных позициях стоят нули, вычисляется так:\n",
    "$$\\mathrm{sim}(x, y, W) = x^TWy.$$\n",
    "\n",
    "Для обучения матрицы $W$ можно использовать функции потерь из следующего параметрического семейства, зависящего от параметров $\\alpha_1$, ..., $\\alpha_q$:\n",
    "$$l(x_i, y_i, W) = \\sum_{k = 1}^{\\mathrm{rank}_W(y_i) - 1}\\alpha_k,$$\n",
    "где если верхний предел суммы равен 0, то сумма по пустому множеству индексов полагается равной нулю, а зависимость правой части от $x_i$ заложена в определении $\\mathrm{rank}_W(y_i)$. Эта величина вычисляется как порядковый номер класса $y_i$ при ранжировании классов по невозрастанию $f(y) = \\mathrm{sim}(x_i, y, W)$. В формулировке, учитывающей возможность совпадающих значений, это выглядит так:\n",
    "$$\\mathrm{rank}_W(y_i) = \\#\\!\\left\\{\\left. y \\in Q \\: \\right| \\: x_i^TWy > x_i^TWy_i\\right\\} + 1,$$\n",
    "где $Q$ — $q$-элементное множество классов, а решётка обозначает количество элементов в множестве.\n",
    "\n",
    "Некоторые известные метрики оптимизируются функциями потерь из данного семейства:\n",
    "* точность (accuracy) будет оптимизирована при $\\alpha_1 = 1$ и $\\alpha_k = 0$ для $k \\in \\{2, ..., q\\}$;\n",
    "* вероятность угадать с $m$ попыток будет оптимизирована при $\\alpha_m$ = 1 и остальных $\\alpha_k = 0$;\n",
    "* средний ранг будет оптимизирован при всех $\\alpha_k$, равных друг другу."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "нейронные_сети",
     "анализ_последовательностей"
    ]
   },
   "source": [
    "## Внутреннее устройство LSTM-нейрона\n",
    "\n",
    "#### Что означает LSTM?\n",
    "\n",
    "Аббревиатура LSTM расшифровывается как Long Short-Term Memory (краткосрочная память произвольной длины). Так называется архитектура нейронной сети, часто используемая при решении задач, где объекты образуют последовательность, причём влияние некоторого объекта может распространяться на целевые переменные объектов, стоящих впереди него на произвольном расстоянии.\n",
    "\n",
    "#### Содержимое LSTM-нейрона\n",
    "\n",
    "Отличием LSTM является то, что в ней вводится специальный нейрон со сложной внутренней структурой, называемый LSTM-нейроном или LSTM-модулем (LSTM cell, LSTM unit, LSTM neuron). Рассмотрим устройство такого нейрона.\n",
    "\n",
    "Для каждого индекса $t$, которым нумеруются позиции последовательности, с LSTM-нейроном ассоциированы следующие сущности:\n",
    "* входы и выходы:\n",
    "    - вектор входов $x_t$, берущийся из последовательности и имеющий длину $d$,\n",
    "    - вектор возвращаемых нейроном выходов $y_t$, имеющий длину $h$;\n",
    "* внутреннее состояние:\n",
    "    - вектор \"памяти\" $c_t$ той же длины $h$, что и вектор выходов;\n",
    "* три вектора, называемых вентилями и имеющих ту же длину $h$, что и выход:\n",
    "    - входной вентиль (input gate), $i_t$,\n",
    "    - вентиль забвения (forget gate), $f_t$,\n",
    "    - выходной вентиль (output gate), $o_t$;\n",
    "* восемь матриц весов и четыре векторных свободных члена:\n",
    "    - $W_i$, $W_f$, $W_o$, $W_c$, их размер $h \\times d$,\n",
    "    - $U_i$, $U_f$, $U_o$, $U_c$, их размер $h \\times h$,\n",
    "    - $b_i$, $b_f$, $b_o$, $b_c$, их длина $h$.\n",
    "    \n",
    "Когда нейронная сеть уже обучена, последние двенадцать сущностей не меняются от шага к шагу, и именно поэтому индекс $t$ у них отсутствует.\n",
    "\n",
    "#### Динамика при движении по последовательности\n",
    "\n",
    "Начальные условия в позиции $t = 0$, предшествующей началу последовательности, задаются так:\n",
    "* $c_0 = 0$, $y_0 = 0$,\n",
    "* веса матриц и свободных членов как-то инициализированы, если происходит обучение, или уже обучены и зафиксированы, если речь идёт о применении,\n",
    "* чему равны вентили, неважно.\n",
    "\n",
    "При заданной последовательности $x_t$ для любой позиции $t \\ge 1$ можно рекуррентно определить значения всех сущностей, ассоциированных с LSTM-нейроном, по формулам:\n",
    "$$i_t = \\sigma(W_i x_t + U_i y_{t-1} + b_i),$$\n",
    "$$f_t = \\sigma(W_f x_t + U_f y_{t-1} + b_f),$$\n",
    "$$o_t = \\sigma(W_o x_t + U_o y_{t-1} + b_o),$$\n",
    "$$c_t = f_t \\circ c_{t-1} + i_t \\circ \\tanh (W_c x_t + U_c y_{t-1} + b_c),$$\n",
    "$$y_t = o_t \\circ \\tanh (c_t),$$\n",
    "где $\\sigma$ — сигмоидная функция активации, а $\\circ$ — операция взятия адамарова произведения (поэлементное умножение).\n",
    "\n",
    "#### Интерпретация\n",
    "\n",
    "Почему данная конструкция, после обучения являющаяся некоторой динамической системой в дискретном времени, осмысленна?\n",
    "\n",
    "Можно считать, что LSTM-нейрон в зависимости от значений вентилей способен демонстрировать различные режимы поведения (ниже 0 и 1 обозначают векторы из одних 0 или 1 соответственно):\n",
    "* при $f_t = 0$, $i_t = o_t = 1$ получается обычный рекуррентный нейрон (нейрон Элмана);\n",
    "* при $o_t = 0$, $i_t = f_t = 1$ происходит накопление входной информации (например, нейрон может работать счётчиком), а на выход информация не отдаётся;\n",
    "* при $i_t = o_t = 0$, $f_t = 1$ происходит хранение того, что в памяти, без изменений;\n",
    "* при $i_t = 0$, $f_t = o_t = 1$ происходит извлечение наружу информации, накопившейся в памяти;\n",
    "* при $i_t = f_t = 0$ происходит сбрасывание памяти.\n",
    "\n",
    "Таким образом, LSTM-нейрон обладает способностями к гибкому управлению памятью и способен проводить над ней все операции, кажущиеся необходимыми. Если данных много и закономерности в них устойчивые, то нейронная сеть с такими нейронами сможет выучить их, даже в случае отсутствия ограничения максимальной дальности влияния входов на выходы.\n",
    "\n",
    "#### Обучение\n",
    "\n",
    "Обучение LSTM-нейрона производится методом, по-английски называемым Truncated Back-Propagation Through Time, где первое слово обозначает, что в вычислениях следующие частные производные полагаются равными нулю:\n",
    "$$\\frac{\\partial i_t}{\\partial y_{t-1}} = \\frac{\\partial f_t}{\\partial y_{t-1}} = \\frac{\\partial o_t}{\\partial y_{t-1}} = \\frac{\\partial c_t}{\\partial y_{t-1}} = 0.$$\n",
    "\n",
    "#### Модификации\n",
    "\n",
    "Помимо классического LSTM-нейрона, описанного выше, существует версия, называемая LSTM с глазками (peepholes). Её отличие в том, что вентили могут обращаться к памяти нейрона (подглядывать в неё через глазки). Вводятся ещё три матрицы размера $h \\times h$, обозначаемые как $V_i$, $V_f$ и $V_o$. В рекуррентных соотношениях, задающих динамику при движении по последовательности, последние два равенства остаются неизменными, а первые три принимают вид:\n",
    "$$i_t = \\sigma(W_i x_t + U_i y_{t-1} + V_i c_{t-1} + b_i),$$\n",
    "$$f_t = \\sigma(W_f x_t + U_f y_{t-1} + V_f c_{t-1} + b_f),$$\n",
    "$$o_t = \\sigma(W_o x_t + U_o y_{t-1} + V_o c_{t} + b_o).$$\n",
    "В последнем из соотношений используется $c_t$, а не $c_{t-1}$, потому что на момент обновления значения выходного вентиля значение памяти на текущем шаге уже известно.\n",
    "\n",
    "Есть и версия LSTM с глазками, где матриц $U_i$, $U_f$, $U_o$ и $U_c$ нет (т.е. можно считать, что они тождественно нулевые)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "нейронные_сети",
     "обработка_текстов"
    ]
   },
   "source": [
    "## На пути к word2vec\n",
    "\n",
    "Помимо word2vec есть некоторые более простые способы отобразить слово в непрерывный вектор.\n",
    "\n",
    "#### Вложение на базе разбивки по коллекциям слов\n",
    "\n",
    "* Построим матрицу, где строкам соответствуют слова, столбцам — некоторые коллекции слов (например, документы или предложения), а на пересечении строки и столбца может стоять:\n",
    "    - бинарный индикатор вхождения слова в коллекцию;\n",
    "    - количество вхождений слова в коллекцию;\n",
    "    - TF-IDF;\n",
    "    - BM-25.\n",
    "* Применим к получившейся матрице какую-либо процедуру снижения размерности, такую как:\n",
    "    - метод главных компонент, PCA;\n",
    "    - многомерное шкалирование, MDS;\n",
    "    - вложение на базе случайного соседства, t-SNE.\n",
    "* Примем за векторное представление некоторого слова соответствующую этому слову строку матрицы со сниженной размерностью.\n",
    "\n",
    "#### Вложение через переходные вероятности\n",
    "\n",
    "* Пронумеруем все слова.\n",
    "* Возьмём матрицу размера $\\vert V \\vert \\times \\vert V \\vert$, где $\\vert V \\vert$ — количество слов, на пересечении $i$-й строки и $j$-го столбца которой стоит вероятность того, что сразу же после $i$-го слова в тексте встретится $j$-е слово. Казалось бы, вложением $i$-го слова можно считать $i$-ю строку такой матрицы или $i$-й столбец такой матрицы, однако у столь простого подхода есть два недостатка:\n",
    "    - Большое количество оцениваемых по данным параметров, а именно $\\vert V \\vert^2$ элементов матрицы переходных вероятностей;\n",
    "    - Размерность вектора вложения такая же, как у вектора, получаемого через one-hot encoding;\n",
    "    - Подобное представление не очень информативно, хотя и более информативно, чем представление, получаемое через one-hot encoding.\n",
    "* Попробуем получить более разумное вложение за счёт большей компактности представления. Предположим, что для некоторого фиксированного $d < \\vert V \\vert$ откуда-то даны две матрицы $W_1$ и $W_2$ размеров $d \\times \\vert V \\vert$ и $\\vert V \\vert \\times d$ соответственно, такие что матрицу переходных вероятностей удалось приблизить матрицей $W_2 W_1$. Последнее предположение формализуется и уточняется так: $\\forall i, i \\in \\{1, ..., \\vert V \\vert\\}$, после применения операции softmax к $\\vert V \\vert \\times 1$ вектору-столбцу $W_2 W_1 x_i$, где $x_i$ — вектор-столбец размера $\\vert V \\vert \\times 1$, такой что его $i$-я компонента равна 1, а остальные равны 0 (т.е. $x_i$ — one-hot представление $i$-го слова), получается вектор, близкий к вектору переходных вероятностей из $i$-го слова. В таком случае вложением слова в векторное пространство можно считать что угодно из этого списка:\n",
    "    - $i$-й столбец матрицы $W_1$;\n",
    "    - $i$-ю строку матрицы $W_2$;\n",
    "    - их полусумму (строго говоря, строку или столбец для этого нужно транспонировать);\n",
    "    - их конкатенацию (опять же, строку или столбец необходимо транспонировать).\n",
    "* Описанный в предыдущем пункте подход имеет следующие преимущества:\n",
    "    - Всего $2 d \\vert V \\vert$ обучаемых параметров;\n",
    "    - Размерность вектора вложения можно сделать маленькой;\n",
    "    - Получаются более информативные векторные представления, поскольку теперь $\\vert V \\vert \\times \\vert V \\vert$ матрица переходных вероятностей не может быть произвольной, а должна быть сводимой к виду $W_2 W_1$ с точностью до применения операции softmax к каждому столбцу указанного произведения двух матриц.\n",
    "\n",
    "#### Связь word2vec с вложением через переходные вероятности\n",
    "\n",
    "Нейронная сеть word2vec является способом найти описанные в предыдущем разделе матрицы $W_1$ и $W_2$, и в этом способе в связи с особенностями обучения нейронных сетей возникают различные специальные трюки наподобие отрицательного сэмплирования. Главное отличие в постановке задачи машинного обучения заключается в том, что в word2vec выходное слово уже не обязано идти сразу же после входного слова, а должно лишь попасть в контекст определённой ширины.  \n",
    "\n",
    "Впрочем, иногда считают, что также в word2vec вводится зависимость структуры нейронной сети от выбранного размера контекстного окна. Обозначим размер контекстного окна за $C$. Тогда:\n",
    "* в word2vec-CBoW вход имеет длину $C \\vert V \\vert$ и в скрытом слое усредняются $C$ произведений одной и той же матрицы $W_1$ со входными one-hot векторами, каждый из которых имеет длину $\\vert V \\vert$;\n",
    "* в word2vec-SkipGram выход имеет длину $C \\vert V \\vert$ и для этих $C$ блоков, представляющих вероятности попадания слова в какую-либо из позиций контекста входного слова, происходит умножение одной и той же матрицы $W_2$ на вектор скрытого слоя (то, что во всех $C$ копиях будет одно и то же вероятностное распределение, нестрашно, ведь задача не в том, чтобы предсказать контекст, а в том, чтобы обучить вложение).\n",
    "\n",
    "Так или иначе, есть и подход к word2vec, где вход и выход имеют длину $\\vert V \\vert$, то есть размеры слоёв не зависят от размера контекстного окна."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "нейронные_сети",
     "обработка_текстов"
    ]
   },
   "source": [
    "## Вероятностная интерпретация word2vec\n",
    "\n",
    "Пусть есть выборка пар ($x_i$, $y_{i,c}$), где $i \\in \\{1, ..., L\\}$, $c \\in \\{1, ..., C\\}$, $C$ — как-то заданное количество слов в контексте, $x_i$ — one-hot encoded вектор слова, а $y_{i,c}$ — one-hot encoded вектор слова, встретившегося в контексте слова $x_i$ на позиции $c$.\n",
    "\n",
    "Допустим, эта выборка порождена из распределения, такого что условные вероятности выхода при условии входа имеют вид $$\\mathbb{P}(y_{i,c} \\vert x_i) = \\frac{\\exp(I(x_i)^TO(y_{i,c}))}{\\sum_{v \\in V}\\exp(I(x_i)^TO(v))},$$\n",
    "где $V$ — множество всех слов, а $I$ и $O$ — некоторые отображения из $\\vert V \\vert$-мерного в $d$-мерное пространство. Сделаем два наблюдения, касающихся выписанной формулы. Во-первых, в правой части применяется операция softmax. Во-вторых, поскольку все векторы $x_i$ и $y_{i,c}$ таковы, что в них на одной позиции стоит 1, а на всех остальных стоят 0, без ограничения общности можно считать, что $I$ и $O$ задаются умножением на матрицы размера $d \\times \\vert V \\vert$.\n",
    "\n",
    "Попытаемся найти эти матрицы, решив задачу максимизации правдоподобия имеющейся выборки:\n",
    "$$\\Pi_{i = 1}^L\\Pi_{c = 1}^C \\mathbb{P}(y_{i,c} \\vert x_i) \\rightarrow \\max_{I, O}.$$\n",
    "\n",
    "Оказывается, word2vec именно эту задачу и решает. Убедимся в этом.\n",
    "\n",
    "Неоптимизированная (слишком долго обучающаяся) word2vec принимает на вход one-hot encoded вектор слова, умножает его слева на $d \\times \\vert V \\vert$ матрицу $W_1$ (по сути, это операция взятия соответствующего слову столбца матрицы $W_1$) и делает результат вектором значений скрытого слоя, а этот вектор значений скрытого слоя умножает слева на $\\vert V \\vert \\times d$ матрицу $W_2$, чтобы получить вектор размера $\\vert V \\vert \\times 1$, после применения операции softmax к которому получаются вероятности попадания слов в контекст входного слова. Раз так, то отображением $I$, выучиваемым word2vec, является умножение слева на матрицу $W_1$, а отображением $O$ — умножение слева на транспонированную матрицу $W_2$. Выкладкой проверяется, что задача обучения неоптимизиованной word2vec с кросс-энтропией в качестве функции потерь совпадает с задачей максимизации выписанного правдоподобия.\n",
    "\n",
    "Небольшой вопрос: почему для слова с идентификатором $j$ выучивается целых два вложения, а именно $j$-й столбец матрицы $W_1$ и $j$-я строка матрицы $W_2$? Дело в том, что положить соответствующие пары весов по определению равными друг другу, чтобы при обучении они учитывались как одна переменная, нельзя, ведь тогда word2vec станет похожей на автокодировщик, и, как следствие, будет завышать вероятность того, что в контексте какого-либо слова встретится оно же само. Это видно и из вероятностной модели — скалярное произведение $I(x_i)^TO(y_{i,c})$ тем больше, чем более коллинеарны множители, а при $I \\equiv O$ это создаёт смещение в пользу $x_i$.\n",
    "\n",
    "Однако обнаруживается, что вышеописанная архитектура word2vec обучается чрезвычайно медленно, так как:\n",
    "* для вложения $\\vert V \\vert$ слов в $d$-мерное пространство требуется на каждом шаге обновлять $2 \\vert V \\vert d$ весов;\n",
    "* на каждом объекте вычисляется softmax от вектора длины $\\vert V \\vert$.\n",
    "\n",
    "Есть два способа ускорить обучение word2vec:\n",
    "* Noise Contrastive Estimation. В нём применяется отрицательное сэмплирование. Сделать его можно двумя способами:\n",
    "    - задача $\\vert V \\vert$-классовой классификации заменяется на задачу бинарной классификации, где от нейронной сети требуется по паре слов сказать, встретились ли они в одном контекстном окне или нет, а для каждого примера положительного класса, взятого из реальных текстов, при том же входном слове сэмплируется несколько выходных слов отрицательного класса, причём сэмплирование производится из эмпирического распределения слов, возведённого в степень 0,75, чтобы редкие слова выбирались чаще.\n",
    "    - рассматривается задача $\\vert V \\vert$-классовой классификации, но при обучении считаются константами все веса от входов к скрытому слою кроме весов, исходящих от текущего входного слова, и все веса от скрытого слоя к выходному слою кроме весов, ведущих к текущему выходному слову, и весов, ведущих к скольки-то другим словам отрицательно просэмплированным по процедуре, описанной в предыдущем пункте.\n",
    "* Hierarchical Softmax. В нём отображение $O$ берётся таким, что областью его определения являются не one-hot encoded векторы слов, а узлы дерева Хаффмана, листьями которого являются слова. В вероятностную модель при этом вносятся соответствующие правки:\n",
    "$$\\mathbb{P}(y_{i,c} \\vert x_i) = \\Pi_{l \\in \\mathrm{Path}(y_{i,c})} \\sigma\\!\\left(-\\mathrm{Child}(l, y_{i,c}) I(x_i)^TO(l)\\right),$$\n",
    "где $\\mathrm{Path}(y_{i,c})$ — путь от корня дерева Хаффмана до листа, соответствующего слову $y_{i,c}$, $\\sigma$ — сигмоидная функуция, а $\\mathrm{Child}(l, y_{i,c})$ равна 1, если в узле $l$ путь до вершины $y_{i,c}$ поворачивает направо, и -1 иначе."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "нейронные_сети",
     "методы_оптимизации"
    ]
   },
   "source": [
    "## Метод подбора темпа обучения нейронной сети Adam\n",
    "\n",
    "В классическом варианте градиентного спуска темп обучения один и тот же для всех настраиваемых весов. Исторически одним из первых методов, где это стало не так, был AdaGrad, главный недостаток которого оказался исправлен в методе под названием RMSProp. А метод, в полном виде называющийся Adaptive Moment Estimation и обычно сокращённо называемый Adam, считается дальнейшим развитием адаптивного управления темпом обучения.\n",
    "\n",
    "Метод Adam имеет четыре гиперпараметра: $\\eta$, $\\beta_1$, $\\beta_2$ и $\\varepsilon$. Гиперпараметр $\\eta$ задаёт относительную скорость обучения (все адаптивно подобранные темпы обучения содержат его как множитель). Гиперпараметр $\\beta_1$ отвечает за сглаживание/угасание накопленного среднего градиентов, а гиперпараметр $\\beta_2$ — за сглаживание/угасание накопленного второго момента градиентов, то есть накопленного среднего их квадратов. Наконец, $\\varepsilon$ служит для численной стабильности.\n",
    "\n",
    "Положим, что по аналогии с методом RMSProp формула обновления некого параметра $\\theta$ (то есть одного из весов какого-либо слоя нейронной сети) на $t$-м шаге обучения имеет вид:\n",
    "$$\\theta_{t+1} := \\theta_t - \\frac{\\eta}{\\sqrt{v_t} + \\varepsilon}m_t,$$\n",
    "где $\\theta_t$ — текущее значение параметра $\\theta$, $\\theta_{t+1}$ — его обновлённое значение, а $m_t$ и $v_t$ — текущие значения присущих алгоритму оптимизации внутренних параметров.\n",
    "\n",
    "Эти два параметра алгоритма оптимизации $m_t$ и $v_t$ обновляются на каждом шаге по следующим правилам простого экспоненциального сглаживания:\n",
    "$$m_t = \\beta_1 m_{t-1} + (1 - \\beta_1) g_{\\theta},$$\n",
    "$$v_t = \\beta_2 v_{t-1} + (1 - \\beta_2) g^2_{\\theta},$$\n",
    "где $g_{\\theta}$ — градиент для параметра нейронной сети $\\theta$ на $t$-м шаге обучения. Что касается формулы для $m_t$, экспоненциальное сглаживание в ней можно сравнить с инерцией, но только это уже инерция темпа обучения, а не инерция обновлений весов, часто используемая при обучении нейронных сетей. Что же касается формулы для $v_t$, в ней экспоненциальное сглаживание нужно не для повышения устойчивости, а для того чтобы накопленная сумма квадратов градиентов со временем могла и падать, а не только расти (главный недостаток AdaGrad как раз к тому и сводится, что там она никогда не падает).\n",
    "\n",
    "Начальными условиями, с которых начинается эволюция во времени параметров алгоритма оптимизации $m_t$ и $v_t$, являются равенства их обоих нулю:\n",
    "$$m_0 = 0,$$\n",
    "$$v_0 = 0.$$\n",
    "Раз так, то значения $m_t$ и $v_t$ при малых $t$ будут смещены в сторону нуля. Чтобы исправить это, вводят следующие скорректированные параметры:\n",
    "$$\\hat{m_t} = \\frac{m_t}{1 - \\beta_1^t},$$\n",
    "$$\\hat{v_t} = \\frac{v_t}{1 - \\beta_2^t}.$$\n",
    "\n",
    "С учётом всего вышесказанного можно выписать окончательную формулу обновления весов методом Adam:\n",
    "$$\\theta_{t+1} := \\theta_t - \\frac{\\eta}{\\sqrt{\\hat{v_t}} + \\varepsilon}\\hat{m_t}.$$\n",
    "\n",
    "Напоследок же стоит заметить, что все методы наподобие AdaGrad, RMSProp или Adam предназначены для ускорения обучения, а не для увеличения качества обученной нейронной сети. В статье [Wilson et al, 2017](https://arxiv.org/abs/1705.08292) показывается, что иногда нейронные сети, обученные ими, проигрывают нейронным сетям, обученным классическим градиентным спуском или стохастическим градиентным спуском. Таким образом, если данных мало, то стоит попробовать не только Adam, но и обычный градиентный спуск."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "нейронные_сети"
    ]
   },
   "source": [
    "## Декомпозиция построения и обучения нейронной сети\n",
    "\n",
    "В построении и обучении нейронных сетей есть три ключевых составляющих:\n",
    "\n",
    "* __Модель__, то есть архитектура и структура нейронной сети:\n",
    "    - Сколько слоёв?\n",
    "    - Какой ширины каждый из слоёв?\n",
    "    - Какие нейроны (юниты) используются?\n",
    "        - обычные,\n",
    "        - с пакетной нормализацией,\n",
    "        - LSTM-модули,\n",
    "        - etc;\n",
    "    - Какие связи между слоями?\n",
    "        - полные,\n",
    "        - локальные (то есть каждый нейрон связан лишь с некоторой областью предыдущего слоя),\n",
    "        - свёрточные (похожи на локальные, но веса в фильтре одинаковые для всего слоя),\n",
    "        - рекуррентные,\n",
    "        - etc;\n",
    "    - Какие функции активации используются в каждом из слоёв?\n",
    "        - нет активации,\n",
    "        - сигмоида,\n",
    "        - гиперболический тангенс,\n",
    "        - ReLU и его модификации, такие как Leaky ReLU или PReLU,\n",
    "        - etc.\n",
    "    - Есть ли какие-либо связанные веса (shared weights)?\n",
    "\n",
    "\n",
    "* __Функция потерь__ (должна быть дифференцируемой почти везде):\n",
    "    - Для регрессии:\n",
    "        - MSE, среднеквадратичная ошибка,\n",
    "        - MAE, средний модуль ошибки,\n",
    "        - etc.\n",
    "    - Для классификации:\n",
    "        - логарифмическая функция потерь (связана с понятием кросс-энтропии, её минимизация эквивалентна минимизации дивергенции Кульбака-Лейблера),\n",
    "        - etc.\n",
    "    - Для других задач:\n",
    "        - GAN loss,\n",
    "        - etc.\n",
    "\n",
    "\n",
    "* __Оптимизатор__, то есть способ по данным настроить все параметры сети:\n",
    "    - На базе градиентного спуска с таким приёмом, как обратное распространение ошибок (этот приём сильно ускоряет вычисление градиента):\n",
    "        - Когда обновлять веса?\n",
    "            - После подсчёта коррекций на всех объектах обучающей выборки (классический градиентный спуск),\n",
    "            - После подсчёта коррекций на всём пакете из случайно выбранных объектов обучающей выборки (пакетный градиентный спуск),\n",
    "            - После подсчёта коррекций на каждом отдельно взятом объекте (стохастический градиентный спуск).\n",
    "        - Какой темп обучения (learning rate) выбрать?\n",
    "            - Постоянный,\n",
    "            - С расписанием, то есть в виде заранее заданной невозрастающей функции от числа шагов,\n",
    "            - Адаптивный:\n",
    "                - AdaGrad,\n",
    "                - RMSProp,\n",
    "                - Adam.\n",
    "        - Какую инерцию (momentum) обновлений весов выбрать?\n",
    "            - никакую,\n",
    "            - обычную,\n",
    "            - инерцию Нестерова.\n",
    "        - Как инициализировать веса каждого из слоёв перед началом обучения?\n",
    "            - Случайно породить $I \\times O$ весов, где $I$ — размер входа слоя, а $O$ — размер выхода слоя:\n",
    "                - Из гауссовского распределения с нулевым средним и дисперсией:\n",
    "                    - 0.01 (или ещё меньше, если всё равно обучение не идёт),\n",
    "                    - $2 / (I + O)$ (Glorot Normal Initialization),\n",
    "                    - $1/I$ (часто используется для функции активации $\\tanh$),\n",
    "                    - $2/I$ (He Normal Initialization, часто используется для функции активации ReLU);\n",
    "                - Из равномерного распределения на отрезке $[-a, a]$, где $a$ равно:\n",
    "                    - $\\sqrt{6 / (I + O)}$ (Glorot Uniform Initialization),\n",
    "                    - $\\sqrt{3 / I}$ (LeCun Uniform Initialization).\n",
    "            - Жадно послойно предобучить с дополнением до автокодировщика.\n",
    "        - Какие техники регуляризации использовать?\n",
    "            - $L_1$-регуляризация (приводит к отбору весов),\n",
    "            - $L_2$-регуляризация (приводит к затуханию весов),\n",
    "            - дропаут,\n",
    "            - подмешивание шума.\n",
    "        - Использовать ли раннюю остановку, отталкивающуюся от оценки качества на отложенной выборке?\n",
    "    - На базе генетических алгоритмов (не мэйнстрим в обучении нейронных сетей)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "активное_обучение",
     "постановка_задачи"
    ]
   },
   "source": [
    "## Оценивание дисперсии целевой переменной\n",
    "\n",
    "Если функцией потерь, с которой обучается регрессор, является среднеквадратичная ошибка (MSE), то регрессор будет пытаться предсказать условное среднее целевой переменной при условии признаков. Если взять средний модуль ошибки (MAE), то тогда получится оценка условной медианы целевой переменной при условии признаков. Оценка условной дисперсии целевой переменной при условии признаков тоже может быть получена, хотя это и не достигается за счёт выбора подходящей функции потерь. Нужна же оценка условной дисперсии, например, тогда, когда есть возможность дополнительно запрашивать примеры в обучающую выборку — из областей признакового пространства, где дисперсия выше, хочется иметь больше примеров, чем из областей признакового пространства, где дисперсия ниже.\n",
    "\n",
    "Для получения оценки условной дисперсии применим следующий эвристический подход:\n",
    "* С MSE в качестве функции потерь обучим модель с исходной целевой переменной;\n",
    "* С MSE в качестве функции потерь обучим модель, для которой целевой переменной является квадрат исходной целевой переменной;\n",
    "* Предсказанием дисперсии на некотором объекте назовём разность предсказания второй модели и предсказания первой модели, если эта разность больше 0, и 0 иначе.\n",
    "\n",
    "Описанный подход опирается на определение дисперсии как разности математического ожидания квадрата и квадрата математического ожидания."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "постановка_задачи"
    ]
   },
   "source": [
    "## Перенос модели из одной среды в другую\n",
    "\n",
    "Предположим, что на некотором наборе признаков уже обучена модель, но все данные, из которых была составлена обучающая выборка, приходили из одной среды, а теперь хочется решать аналогичную задачу для объектов, приходящих из другой среды, где доступны только признаковые описания, но не целевые переменные.\n",
    "\n",
    "Примеры подобных ситуаций таковы:\n",
    "* Для решения задачи кредитного скоринга банк, работавший в некотором регионе, собрал достаточно размеченных примеров с жителями этого региона. Теперь банк хочет открыть отделение в другом регионе, однако там за короткое время можно собрать только заявки на кредит с признаками, но без целевой переменной, ведь для того, чтобы узнать, будет ли кредит возвращён, нужно ждать несколько лет.\n",
    "* Мобильное приложение с моделью монетизации freemium (установка и базовое использование бесплатны, платны дополнительные возможности), долгое время выпускавшееся лишь под одну платформу, получило версию под другую платформу. Маркетологи хотят получать предсказание будущих поступлений от пользователей, привлекаемых в версию под новую платформу той или иной рекламной кампанией. Прямо сейчас доступны только параметры аудиторий и рекламных кампаний, но не конечный результ, ведь для его измерения необходимо ждать время, равное некой высокой квантили срока активности пользователя в приложении.\n",
    "\n",
    "До тех пор, пока в новой среде не собрано достаточное количество размеченных данных, можно задействовать урезанную версию модели, обученную на данных из старой среды. Строится она так:\n",
    "* Признаковые описания объектов из обеих сред вертикально конкатенируются, а новой целевой переменной становится бинарная переменная, равная 0, если объект пришёл из одной среды, и 1, если из другой.\n",
    "* Обучается классификатор примерно той же природы, что и исходная модель, и смотрится, какие признаки обладают для него наибольшей важностью.\n",
    "* Ищется подмножество признаков, такое что на нём классификатор не может показать хорошие метрики качества. Это подмножество признаков — подмножество таких признаков, что они с точки зрения моделей, похожих на выбранный классификатор, в совокупности выглядят одинаково в обеих средах.\n",
    "* На отобранном подмножестве признаков только на объектах из старой среды обучается модель для исходной задачи, и именно эту модель можно использовать для предсказаний на объектах из новой среды.\n",
    "\n",
    "Описанный приём позволяет избежать ошибок экстраполяции, связанных с тем, что модель применяется в ранее не виденных ей областях признакового пространства. В то же время использование лишь части признаков приводит к некоторому снижению качества модели. Стало быть, нужно найти оптимальный баланс между качеством модели и степенью различия между средами."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "нейронные_сети"
    ]
   },
   "source": [
    "## Пакетная нормализация\n",
    "\n",
    "Слои нейронов с пакетной нормализацией имеют то преимущество перед обычными слоями, что обучаются быстрее, а ещё им сопутствует некоторая дополнительная регуляризация, благодаря которой порой не нужен дропаут. Правда, обучаться слои с пакетной нормализацией могут только пакетным градиентным спуском.\n",
    "\n",
    "Везде далее речь пойдёт о нейроне слоя с пакетной нормализацией, а не о целом слое, хотя то же самое можно сформулировать и в терминах слоя.\n",
    "\n",
    "При пакетной нормализации помимо обычных весов с каждым нейроном ассоциированы следующие дополнительные сущности:\n",
    "* Два скаляра, обучаемых пакетным градиентных спуском:\n",
    "    - $\\gamma$, скаляр перемасштабирования, инициализируется 1,\n",
    "    - $\\beta$, скаляр сдвига, инициализируется 0;\n",
    "* Два скаляра, существующих только во время обучения:\n",
    "    - $\\mu_\\mathrm{batch}$, среднее по текущему пакету от значений, подаваемых непосредственно в функцию активации нейрона (то есть от значений, возникающих после умножения входов на соответствующие веса, но до взятия функции активации от получившейся суммы),\n",
    "    - $\\sigma_\\mathrm{batch}$, среднеквадратичное отклонение значений, подаваемых непосредственно в функцию активации нейрона, подсчитанное по текущему пакету;\n",
    "* Два скаляра, арифметическими операциями обновляемые на каждом пакете во время обучения и остающиеся константами во время применения, нужны они только на стадии применения:\n",
    "    - $\\mu_\\mathrm{cum}$, экспоненциальное среднее от средних по пакетам значений, подаваемых непосредственно в функцию активации нейрона,\n",
    "    - $\\sigma_\\mathrm{cum}$, экспоненциальное среднее от внутрипакетных среднеквадратичных отклонений значений, подаваемых непосредственно в функцию активации нейрона.\n",
    "    \n",
    "К числу гиперпараметров пакетная нормализация добавляет только коэффициент сглаживания $\\alpha$ для подсчёта экспоненциальных средних (или несколько таких коэффициентов для разных слоёв и отдельно для средних и среднеквадратичных отклонений).\n",
    "   \n",
    "Идея, стоящая за пакетной нормализацией, восходит к тому, что в машинном обучении бывает полезно центрировать данные и делать их дисперсию единичной. В контексте нейронных сетей это особенно актуально, если функцией активации является сигмоида или гиперболический тангенс, потому что у них градиенты заметно отличаются от нуля лишь в окрестности нуля. Также в случае глубоких нейронных сетей вычитание среднего и деление на дисперсию не могут быть частью предобработки данных, а должны быть частью процесса обучения, потому что до слоёв, расположенных далеко от входных, эффект от разовой предобработки может и не дойти.\n",
    "\n",
    "Обучение нейрона, с которым на текущий момент ассоциирован вектор-строка весов $w$ и которому в пакете подаются векторы-столбцы $x_i$, где $i$ принимает значения от 1 до размера пакета, выглядит так:\n",
    "* Вычислить все $u_i = wx_i$ (свободный член отсутствует, потому что его он избыточен в случае пакетной нормализации, ведь его заменяет $\\beta$);\n",
    "* Вычислить $\\mu_\\mathrm{batch}$ как среднее $u_i$ и вычислить $\\sigma_\\mathrm{batch}$ как среднеквадратичное отклонение $u_i$;\n",
    "* Вычислить нормализованные значения $v_i = (u_i - \\mu_\\mathrm{batch}) \\: / \\: (\\sigma_\\mathrm{batch} + \\varepsilon)$, где $\\varepsilon$ — некоторая малая константа, позволяющая избежать проблем с делением на ноль;\n",
    "* Обновить значения $\\mu_\\mathrm{cum}$ и $\\sigma_\\mathrm{cum}$ по правилу простого экспоненциального сглаживания:\n",
    "$$\\mu_\\mathrm{cum} := \\alpha\\mu_\\mathrm{cum} + (1-\\alpha)\\mu_\\mathrm{batch},$$\n",
    "$$\\sigma_\\mathrm{cum} := \\alpha\\sigma_\\mathrm{cum} + (1-\\alpha)\\sigma_\\mathrm{batch};$$\n",
    "* Вычислить значения функции активации $f_\\mathrm{activ}(\\gamma v_i + \\beta)$ и передать каждое из них как вход для следующего слоя на соответствующем ($i$-м) объекте батча;\n",
    "* При обновлении параметров нейронной сети методом обратного распространения ошибок также считать градиенты по $\\gamma$ и $\\beta$, чтобы градиентный спуск сам мог решить, какими примерно должны быть средние и дисперсии, ведь это влияет на то, какие участки нелинейности функции активации задействованы.\n",
    "\n",
    "На стадии применения для нормализации всегда используются $\\mu_\\mathrm{cum}$ и $\\sigma_\\mathrm{cum}$, а $\\gamma$ и $\\beta$ полагаются константами."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "нейронные_сети",
     "постановка_задачи",
     "теория_информации"
    ]
   },
   "source": [
    "## Логарифмическая функция потерь (log loss) и энтропия Шеннона\n",
    "\n",
    "Логарифмическая функция потерь имеет вид:\n",
    "$$\\mathrm{log\\_loss}\\left(y, \\hat{P}\\right) = -\\frac{1}{N}\\sum_{i = 1}^N\\sum_{j = 1}^C [y_i = j] \\, \\log \\hat{P}_{ij},$$\n",
    "где $N$ — количество объектов, $C$ — количество классов, $y$ — вектор длины $N$, такой что его $i$-я компонента $y_i$ равна верному ответу на $i$-м объекте выборки, $\\hat{P}$ — матрица размера $N \\times C$, такая что на пересечении $i$-й строки и $j$-го столбца стоит предсказанная вероятность принадлежности $i$-го объекта к $j$-му классу, а, наконец, квадратные скобки даны в нотации Айверсона, то есть выражение с ними равно 1, если то, что в скобках, верно, и равно 0, если то, что в скобках, неверно.\n",
    "\n",
    "Логарифмическая функция потерь имеет интерпретацию, восходящую к теории информации. Чтобы понять, какие идеи стоят за логарифмической функцией потерь, обратимся к понятию энтропии Шеннона. Энтропия Шеннона — функционал на вероятностных распределениях. Поскольку речь идёт о $C$-классовой классификации, определим энтропию Шеннона только на вероятностных распределениях на множестве $C$ классов, хотя на самом деле она определена и для непрерывных распределений:\n",
    "$$H(p) = - \\sum_{i = 1}^C p_i \\log_2{p_i},$$\n",
    "где $p = (p_i)_{i=1}^C$ — вектор длины $C$, такой что его $i$-я компонента $p_i$ равна вероятности того, что просэмплированный из генеральной совокупности объект принадлежит к $i$-му классу.\n",
    "\n",
    "Какой смысл в так определённой величине? Короткий ответ: она сообщает, чему равно математическое ожидание (относительно генеральной совокупности) количества бит информации, получаемой при измерении класса одного случайного объекта. Например, если все классы кроме одного имеют нулевую вероятность, то $H(p) = 0$, ведь никакой информации в измерении класса нет: и без измерений известно, что может быть только один класс.\n",
    "\n",
    "Из короткого ответа непонятно, почему энтропия Шеннона, и впрямь, является ожидаемым количеством бит информации, даваемым замером класса на одном случайном объекте. Чтобы углубиться в это, придётся немного формализовать задачу. Допустим, метку каждого класса представили в виде последовательности нулей и единиц (т.е. последовательности бит) так, что у разных классов разные метки. При вышеописанной кодировке введём $l$ как вектор длины $C$, $l = (l_i)_{i=1}^C$, $i$-я компонента которого $l_i$ равна длине последовательности нулей и единиц, кодирующей метку $i$-го класса. Задача ставится как задача минимизации по возможным способам закодировать метки классов (так, чтобы разные классы имели разные метки) следующего функционала, являющегося математическим ожиданием длины последовательности, представляющей класс случайного объекта:\n",
    "$$L = \\sum_{i = 1}^C p_i l_i.$$\n",
    "Шеннон показал, что оптимальное значение $L_{opt}$ не может быть меньше, чем $H(p)$. Более того, существуют схемы, позволяющие приблизиться к этому теоретическому минимуму, а именно коды Хаффмана и коды Шеннона-Фано. Таким образом, становится понятно, почему $H(p)$ можно интерпретировать как ожидаемое количество бит информации, ведь в вышеописанной тракторвке это ожидаемая длина битового представления метки класса при оптимальном кодировании меток.\n",
    "\n",
    "Чтобы понять, как логарифмическая функция потерь связана с энтропией Шеннона, введём понятие кросс-энтропии между двумя задаваемыми векторами $p$ и $q$ вероятностными распределениями на метках классов:\n",
    "$$H(p, q) = - \\sum_{i = 1}^C p_i \\log_2{q_i}.$$\n",
    "Эта величина может восприниматься следующим образом. Допустим, вектор $p$ содержит истинные вероятности классов, но при выборе способа закодировать метки классов думали, что истинные вероятности классов задаются вектором $q$, так что и кодирование получилось оптимальным для $q$. В таком случае $H(p, q)$ задаёт ожидаемое количество бит информации, которую потребуется передать, чтобы с указанным оптимальным для $q$ кодированием сообщить метку класса объекта, пришедшего из распределения, характеризуемого $p$. Только что кросс-энтропия $H(p, q)$ была описана с точки зрения разрастания количества бит, которое потребуется передать, но есть и взгляд с точки зрения потери информации: если разрешено передать лишь столько бит, сколько ровно хватило бы при оптимальном для $p$ кодировании, то при оптимальном для $q$ кодировании часть информации придётся потерять.\n",
    "\n",
    "Так вот, логарифмическая функция потерь — средняя по выборке кросс-энтропия между распределением, сконцентрированным в истинном классе объекта, и распределением, задаваемым предсказанными для этого объекта вероятностями классов."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "теория_информации"
    ]
   },
   "source": [
    "## Дивергенция Кульбака-Лейблера\n",
    "\n",
    "Дивергенция Кульбака-Лейблера — асимметричное \"расстояние\" между двумя вероятностными распределениями. Из-за асимметричности и используется слово \"дивергенция\", а не слово \"расстояние\".\n",
    "\n",
    "В случае распределений над дискретным множеством $C$ возможных классов любое вероятностное распределение можно представить в виде вектора длины $C$, сумма компонент которого равна 1. Тогда для распределений, характеризуемых векторами $p=(p_i)_{i=1}^C$ и $q=(q_i)_{i=1}^C$, дивергенция Кульбака-Лейблера равна:\n",
    "$$D_{KL}\\left(p \\, \\Vert \\, q \\right) = - \\sum_{i=1}^C p_i \\log \\frac{q_i}{p_i}.$$\n",
    "\n",
    "Смысл так введённой величины становится понятен из следующего соотношения:\n",
    "$$D_{KL}\\left(p \\, \\Vert \\, q \\right) = H(p, q) - H(p),$$\n",
    "где $H(p, q)$ — кросс-энтропия между распределениями, описываемыми векторами $p$ и $q$, а $H(p)$ — энтропия Шеннона распределения, описываемого вектором $p$. Из выписанного равенства следует, что дивергенция Кульбака-Лейблера показывает, чему равно ожидаемое количество \"лишних\" бит, которые потребуется передать для описания исхода, приходящего из распределения, описываемого $p$, если для кодирования исходов была выбрана схема, оптимизированная для распределения, описываемого $q$. Также из выписанного равенства следует, что при использовании в качестве функции потерь дивергенции Кульбака-Лейблера между распределением, сконцентрированным в истинном классе объекта, и предсказанными вероятностями классов, получится то же самое, что получается при использовании логарифмической функции потерь, так как $H(p)$ не зависит от предсказанного $q$.\n",
    "\n",
    "Свойства дивергенции Кульбака-Лейблера:\n",
    "* Неотрицательна: $D_{KL}\\left(p \\, \\Vert \\, q \\right) \\ge 0$, причём равна 0 тогда и только тогда, когда $p = q$ (следует из неравенства Йенсена);\n",
    "* С точностью до умножения на константу является предельным элементом параметрического семейства дивергенций Кресси-Рида в пределе при $\\lambda \\to 0$:\n",
    "$$D_{KL}\\left(p \\, \\Vert \\, q \\right) = -\\frac{1}{2} \\lim_{\\lambda \\to 0} \\frac{2}{\\lambda(\\lambda + 1)} \\sum_{i=1}^C p_i\\left(\\left(\\frac{p_i}{q_i}\\right)^\\lambda - 1\\right),$$\n",
    "следует это из правила Лопиталя. К этому же параметрическому семейству с точностью до умножения на константу принадлежат расстояние Хеллингера и $\\chi^2$-расстояние."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "байесовские_методы"
    ]
   },
   "source": [
    "## Байесовский взгляд на EM-алгоритм\n",
    "\n",
    "В задаче кластеризации EM-алгоритм может использоваться для разделения смеси гауссовских распределений, причём он имеет то преимущество перед методом K-средних, что ковариационные матрицы гауссиан не обязаны быть единичными — для отсутствия вычислительной неустойчивости достаточно, чтобы они были диагональными, хотя есть и версии с более слабыми ограничениями. На самом же деле, EM-алгоритм имеет более общий вид, чем алгоритм для разделения смеси гауссовских распределений. Это алгоритм, позволяющий оценивать параметры распределений тогда, когда часть переменных относится к ненаблюдаемым.\n",
    "\n",
    "Формализуем постановку задачи следующим образом. Пусть $x$ — вектор наблюдаемых переменных, $t$ — вектор скрытых переменных, а $\\theta$ — вектор неизвестных параметров модели, которые требуется оценить по данным. Для компактности изложения будем считать, что $t$ — непрерывный вектор длины 1, то есть скаляр, $t \\in \\mathbb{R}$. Если длина $t$ больше, то просто придётся интегрировать вдоль большего количества координатных осей, а если $t$ дискретно, то интегралы заменятся на суммы. Про функцию условной плотности пусть будет известно, что она допускает представление в виде:\n",
    "$$p(x \\, \\vert \\, \\theta) = \\int p(x, t \\, \\vert \\, \\theta) dt,$$\n",
    "где $p(x, t \\, \\vert \\, \\theta)$ дана в аналитическом виде. Наконец, предположим, что безусловное распределение $t$ задаётся некоторой неизвестной функцией плотности $q(t)$. Задача заключается в том, чтобы методом максимального правдоподобия найти оценку для $\\theta$, имея обучающую выборку с наблюдениями $x$.\n",
    "\n",
    "Задача разделения смеси гауссовских распределений является частным случаем описанной задачи, получающимся из неё, если положить, что $t$ — дискретная переменная, задающая идентификатор смеси, $x$ — наблюдаемые координаты объекта, а $\\theta$ — вектор, компонентами которого являются центры и ковариационные матрицы гауссиан (развёрнутые из двумерного в одномерный вид, естественно).\n",
    "\n",
    "Для решения общей задачи предпримем следующие шаги, на каждом из которых будем преобразовывать логарифм плотности в некой точке из обучающей выборки:\n",
    "* $\\log p(x \\, \\vert \\, \\theta) = \\int q(t) \\log p(x \\, \\vert \\, \\theta) dt$, потому что левая часть не зависит от $t$, а справа её просто внесли как константу в интеграл, равный 1;\n",
    "* правую часть только что выписанного равенства представим в виде $\\int q(t) \\log \\frac{p(x, t \\, \\vert \\, \\theta)}{p(t \\, \\vert \\, x, \\theta)} dt$ — сделать это можно по теореме Байеса;\n",
    "* домножим числитель и знаменатель выражения под логарифмом на $q(t)$: $\\int q(t) \\log \\frac{p(x, t \\, \\vert \\, \\theta) \\, q(t)}{p(t \\, \\vert \\, x, \\theta) \\, q(t)} dt$;\n",
    "* воспользуемся тем, что логарифм произведения равен сумме логарифмов, а множители сгруппируем крест-накрест: $\\int q(t) \\log \\frac{p(x, t \\, \\vert \\, \\theta)}{q(t)} dt + \\int q(t) \\log \\frac{q(t)}{p(t \\, \\vert \\, x, \\theta)} dt$.\n",
    "\n",
    "Первое слагаемое обозначим за $\\mathcal{L}(q, \\theta)$:\n",
    "$$\\mathcal{L}(q, \\theta) = \\int q(t) \\log \\frac{p(x, t \\, \\vert \\, \\theta)}{q(t)} dt.$$\n",
    "Отметим, что в правой части присутствует $x$, но он не выписан как аргумент, от которого зависит $\\mathcal{L}(q, \\theta)$. Опустить $x$ можно по следующей причине: $\\log p(x \\, \\vert \\, \\theta)$ рассматривается в контексте задачи максимизации правдоподобия, а значит (из какого бы слагаемого прологарифмированного правдоподобия $\\log p(x \\, \\vert \\, \\theta)$ ни пришёл) $x$ является константным вектором, равным вектору наблюдаемых признаков соответствующего объекта обучающей выборки. Таким образом, $\\mathcal{L}(q, \\theta)$ является функционалом, зависящим от функции плотности $q$ и вектора $\\theta$.\n",
    "\n",
    "Кроме того заметим, что второе слагаемое в получившемся на последнем шаге выражении равно дивергенции Кульбака-Лейблера между неизвестным распределением $q(t)$ и распределением $p(t \\, \\vert \\, x, \\theta)$, вид которого можно получить из известного по условию распределения $p(x, t \\, \\vert \\, \\theta)$:\n",
    "$$D_{KL}(q(t) \\, \\Vert \\, p(t \\, \\vert \\, x, \\theta)) = \\int q(t) \\log \\frac{q(t)}{p(t \\, \\vert \\, x, \\theta)} dt.$$\n",
    "\n",
    "Итак, путём вышеописанных преобразований было получено, что:\n",
    "$$\\log p(x \\, \\vert \\, \\theta) = \\mathcal{L}(q, \\theta) + D_{KL}(q(t) \\, \\Vert \\, p(t \\, \\vert \\, x, \\theta)).$$\n",
    "Второе слагаемое правой части неотрицательно в силу соответствующего свойства дивергенции Кульбака-Лейблера. Из этих соображений первое слагаемое называют нижней вариационной оценкой для $\\log p(x \\, \\vert \\, \\theta)$.\n",
    "\n",
    "Теперь всё готово, для того чтобы описать EM-алгоритм, максимизирующий по $\\theta$ прологарифмированное правдоподобие $\\sum_{x \\in S} \\log p(x \\, \\vert \\, \\theta)$, где $S$ — обучающая выборка:\n",
    "* E-шаг: при фиксированном $\\theta = \\theta_n$ для каждого $x \\in S$ решается задача $\\mathcal{L}(q, \\theta) \\to \\max_{q \\in Q}$, где $Q$ — некоторое заранее выбранное семейство вероятностных распределений. Рассмотрим случай, когда $Q$ — множество всех вероятностных распределений. В этом случае решение задачи получается тогда, когда обнуляется дивергенция Кульбака-Лейблера, поскольку $\\mathcal{L}(q, \\theta)$ не может быть больше $\\log p(x \\, \\vert \\, \\theta_n)$, в рамках текущего шага являющегося известной константой. Значит, решением является распределение $q_{n+1}(t) = p(t \\, \\vert \\, x, \\theta_n) = \\frac{p(x, t \\, \\vert \\, \\theta_n)}{\\int p(x, t \\, \\vert \\, \\theta_n) dt}$, где правая часть (с точностью до взятия интеграла из знаменателя) известна, потому что $x$ известен, $\\theta_n$ известен и по предоположению, сделанному в формулировке задачи, известен аналитический вид $p(x, t \\, \\vert \\, \\theta)$;\n",
    "* М-шаг: при фиксированном $q = q_n$ для каждого $x \\in S$ решается задача $\\mathcal{L}(q_n, \\theta) \\to \\max_{\\theta}$. Вообще говоря, такая оптимизационная задача может быть многоэкстремальной, но если $Q$ — экспоненциальное семейство распределений, то задача выпуклая (максимизируется вогнутая функция).\n",
    "\n",
    "Ограничение $q \\in Q$ бывает полезным ещё и по следующей причине. На E-шаге при вычислении решения требуется взять интеграл $\\int p(x, t \\, \\vert \\, \\theta_n) dt$, но что, если он не берётся? Можно взять как $Q$ нечто более узкое, чем множество всех вероятностных распределений, и проводить приближённые E-шаги. Например, E-шаг всегда можно провести, если $Q$ — множество всех атомарных распределений, сконцентрированных в одной точке: из определения дивергенции Кульбака-Лейблера получится, что решение — это распределение, сконцентрированное в точке моды по $t$ распределения $p(t \\, \\vert \\, x, \\theta_n)$."
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
